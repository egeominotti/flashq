<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>flashQ Architecture: How We Built a 1.9M Jobs/sec Queue Server - flashQ Blog</title>
  <meta name="description" content="Deep dive into flashQ's architecture: sharded design, lock-free data structures, and optimizations that achieve 1.9M jobs/sec throughput.">
  <meta name="keywords" content="flashq architecture, high performance queue, rust job queue, sharded architecture, lock-free data structures">
  <meta name="robots" content="index, follow">

  <meta property="og:title" content="flashQ Architecture: How We Built a 1.9M Jobs/sec Queue Server">
  <meta property="og:description" content="Deep dive into flashQ's architecture and performance optimizations.">
  <meta property="og:type" content="article">
  <meta property="og:url" content="https://flashq.dev/blog/flashq-architecture.html">
  <meta property="og:image" content="https://flashq.dev/og-image.png">

  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="flashQ Architecture: How We Built a 1.9M Jobs/sec Queue Server">
  <meta name="twitter:image" content="https://flashq.dev/og-image.png">

  <link rel="canonical" href="https://flashq.dev/blog/flashq-architecture.html">
  <link rel="icon" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='.9em' font-size='90'>âš¡</text></svg>">

  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800;900&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">

  <link rel="stylesheet" href="styles.css">

  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Article",
    "headline": "flashQ Architecture: How We Built a 1.9M Jobs/sec Queue Server",
    "datePublished": "2026-01-20",
    "author": { "@type": "Organization", "name": "flashQ" }
  }
  </script>

  <!-- Breadcrumb Schema -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "BreadcrumbList",
    "itemListElement": [
      { "@type": "ListItem", "position": 1, "name": "Home", "item": "https://flashq.dev" },
      { "@type": "ListItem", "position": 2, "name": "Blog", "item": "https://flashq.dev/blog/" },
      { "@type": "ListItem", "position": 3, "name": "flashQ Architecture", "item": "https://flashq.dev/blog/flashq-architecture.html" }
    ]
  }
  </script>
</head>
<body>
  <nav>
    <div class="container wide">
      <a href="../" class="logo"><span>âš¡</span> flashQ</a>
      <div class="nav-links">
        <a href="../#features">Features</a>
        <a href="../blog/" class="active">Blog</a>
        <a href="../docs/">Docs</a>
        <a href="https://github.com/egeominotti/flashq" target="_blank">GitHub</a>
        <a href="../docs/#quickstart" class="btn btn-primary">Get Started</a>
      </div>
      <button class="mobile-menu-btn" aria-label="Menu">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
  </nav>

  <div class="mobile-menu">
    <a href="../#features">Features</a>
    <a href="../blog/">Blog</a>
    <a href="../docs/">Docs</a>
    <a href="https://github.com/egeominotti/flashq" target="_blank">GitHub</a>
    <a href="../docs/#quickstart" class="btn btn-primary">Get Started</a>
  </div>

  <header class="article-header">
    <div class="container">
      <span class="article-tag">Deep Dive</span>
      <h1>flashQ Architecture: How We Built a 1.9M Jobs/sec Queue Server</h1>
      <div class="article-meta">
        <span>ğŸ“… January 20, 2026</span>
        <span>â±ï¸ 15 min read</span>
      </div>
    </div>
  </header>

  <article class="article-content">
    <div class="container">
      <p>When we set out to build flashQ, our goal was simple: create the fastest job queue server possible, optimized for AI workloads, without requiring Redis. In this article, we'll dive deep into the architecture decisions that enable flashQ to process <strong>1.9 million jobs per second</strong>.</p>

      <h2>High-Level Architecture</h2>

      <p>flashQ is written in Rust and follows a sharded, lock-free architecture designed for maximum concurrency:</p>

      <pre><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      flashQ Server                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚ TCP/IP  â”‚  â”‚  HTTP   â”‚  â”‚  gRPC   â”‚  â”‚  Unix   â”‚       â”‚
â”‚  â”‚ Handler â”‚  â”‚   API   â”‚  â”‚   API   â”‚  â”‚ Socket  â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜       â”‚
â”‚       â”‚            â”‚            â”‚            â”‚             â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                         â”‚                                   â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚              â”‚   Command Router    â”‚                       â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚                         â”‚                                   â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚         â–¼               â–¼               â–¼                  â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚   â”‚ Shard 0  â”‚   â”‚ Shard 1  â”‚...â”‚ Shard 31 â”‚              â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                         â”‚                                   â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚              â”‚   DashMap Index     â”‚                       â”‚
â”‚              â”‚  (Lock-Free O(1))   â”‚                       â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</code></pre>

      <h2>The 32-Shard Design</h2>

      <p>At the heart of flashQ is a sharded architecture. We distribute queues across <strong>32 shards</strong> based on a hash of the queue name. This reduces lock contention by 97% compared to a single-lock design.</p>

      <pre><code><span class="comment">// Queue name â†’ Shard mapping</span>
<span class="keyword">fn</span> <span class="function">get_shard</span>(queue_name: &<span class="keyword">str</span>) -> <span class="keyword">usize</span> {
    <span class="keyword">let</span> <span class="variable">hash</span> = fxhash::<span class="function">hash64</span>(queue_name.as_bytes());
    (hash <span class="keyword">as</span> <span class="keyword">usize</span>) % <span class="number">32</span>
}</code></pre>

      <p>Each shard contains:</p>

      <ul>
        <li><strong>Queues</strong>: HashMap of queue name â†’ IndexedPriorityQueue</li>
        <li><strong>Processing</strong>: Jobs currently being processed</li>
        <li><strong>Completed</strong>: Recently completed job IDs</li>
        <li><strong>DLQ</strong>: Dead letter queue for failed jobs</li>
        <li><strong>Results</strong>: Job results for the <code>finished()</code> API</li>
      </ul>

      <h2>Lock-Free Job Index with DashMap</h2>

      <p>One of our biggest performance wins came from using <code>DashMap</code> for the global job index. DashMap is a concurrent HashMap that uses fine-grained locking, allowing multiple threads to read and write simultaneously.</p>

      <pre><code><span class="comment">// Global job index - O(1) lookups</span>
<span class="keyword">pub struct</span> <span class="function">QueueManager</span> {
    shards: [RwLock&lt;Shard&gt;; <span class="number">32</span>],
    job_index: DashMap&lt;<span class="keyword">u64</span>, JobLocation&gt;,  <span class="comment">// Lock-free!</span>
}

<span class="comment">// JobLocation tells us exactly where a job is</span>
<span class="keyword">enum</span> <span class="function">JobLocation</span> {
    Waiting { shard: <span class="keyword">u8</span>, queue: CompactString },
    Processing { shard: <span class="keyword">u8</span> },
    Completed { shard: <span class="keyword">u8</span> },
    Failed { shard: <span class="keyword">u8</span> },
}</code></pre>

      <p>This gives us <strong>O(1) job lookups</strong> regardless of how many queues or jobs exist. Operations like <code>getJob()</code>, <code>cancel()</code>, and <code>getState()</code> are blazing fast.</p>

      <h2>IndexedPriorityQueue for O(log n) Operations</h2>

      <p>Standard binary heaps don't support efficient removal of arbitrary elements. We built an <code>IndexedPriorityQueue</code> that maintains a secondary index:</p>

      <pre><code><span class="keyword">pub struct</span> <span class="function">IndexedPriorityQueue</span>&lt;T&gt; {
    heap: Vec&lt;T&gt;,
    index: HashMap&lt;<span class="keyword">u64</span>, <span class="keyword">usize</span>&gt;,  <span class="comment">// job_id â†’ heap position</span>
}

<span class="keyword">impl</span>&lt;T&gt; <span class="function">IndexedPriorityQueue</span>&lt;T&gt; {
    <span class="comment">// All operations are O(log n)</span>
    <span class="keyword">fn</span> <span class="function">push</span>(&<span class="keyword">mut self</span>, job: T) { ... }
    <span class="keyword">fn</span> <span class="function">pop</span>(&<span class="keyword">mut self</span>) -> Option&lt;T&gt; { ... }
    <span class="keyword">fn</span> <span class="function">remove</span>(&<span class="keyword">mut self</span>, job_id: <span class="keyword">u64</span>) -> Option&lt;T&gt; { ... }  <span class="comment">// Key!</span>
    <span class="keyword">fn</span> <span class="function">update_priority</span>(&<span class="keyword">mut self</span>, job_id: <span class="keyword">u64</span>, priority: <span class="keyword">i32</span>) { ... }
}</code></pre>

      <p>This enables efficient <code>cancel()</code>, <code>promote()</code>, and <code>changePriority()</code> operations that would be O(n) with a standard heap.</p>

      <h2>CompactString for Zero-Allocation Queue Names</h2>

      <p>Queue names are accessed constantly. We use <code>CompactString</code> which stores strings up to 24 bytes inline (no heap allocation):</p>

      <pre><code><span class="comment">// Most queue names fit in 24 bytes</span>
<span class="keyword">use</span> compact_str::CompactString;

<span class="comment">// "embeddings" - 10 bytes, stored inline âœ“</span>
<span class="comment">// "user-notifications" - 18 bytes, stored inline âœ“</span>
<span class="comment">// "very-long-queue-name-here" - 25 bytes, heap allocated</span></code></pre>

      <p>This reduces memory allocations by ~60% for typical workloads.</p>

      <h2>Sharded Processing Map</h2>

      <p>Jobs being processed are distributed across 32 shards (separate from queue shards). This reduces contention during <code>ack()</code> and <code>fail()</code> operations by 97%:</p>

      <pre><code><span class="comment">// Processing is sharded by job_id</span>
<span class="keyword">fn</span> <span class="function">get_processing_shard</span>(job_id: <span class="keyword">u64</span>) -> <span class="keyword">usize</span> {
    (job_id <span class="keyword">as</span> <span class="keyword">usize</span>) % <span class="number">32</span>
}

<span class="comment">// ack() only locks one of 32 processing shards</span>
<span class="keyword">fn</span> <span class="function">ack</span>(&<span class="keyword">self</span>, job_id: <span class="keyword">u64</span>) -> Result&lt;()&gt; {
    <span class="keyword">let</span> <span class="variable">shard_idx</span> = <span class="function">get_processing_shard</span>(job_id);
    <span class="keyword">let</span> <span class="keyword">mut</span> <span class="variable">processing</span> = <span class="keyword">self</span>.processing[shard_idx].<span class="function">write</span>();
    <span class="comment">// ...</span>
}</code></pre>

      <h2>Binary Protocol with MessagePack</h2>

      <p>flashQ supports both JSON and MessagePack protocols. MessagePack provides:</p>

      <ul>
        <li><strong>40% smaller</strong> payloads on the wire</li>
        <li><strong>3-5x faster</strong> serialization/deserialization</li>
        <li>Full type safety and schema compatibility</li>
      </ul>

      <pre><code><span class="comment">// TypeScript SDK - enable binary protocol</span>
<span class="keyword">const</span> <span class="variable">client</span> = <span class="keyword">new</span> <span class="function">FlashQ</span>({
  host: <span class="string">'localhost'</span>,
  port: <span class="number">6789</span>,
  useBinary: <span class="keyword">true</span>  <span class="comment">// Use MessagePack</span>
});</code></pre>

      <h2>Memory Management</h2>

      <p>We use several strategies to prevent unbounded memory growth:</p>

      <h3>Completed Jobs Cleanup</h3>
      <pre><code><span class="comment">// When completed_jobs exceeds 50K, remove oldest 25K</span>
<span class="keyword">if</span> completed_jobs.<span class="function">len</span>() > <span class="number">50_000</span> {
    <span class="keyword">let</span> <span class="variable">to_remove</span>: Vec&lt;_&gt; = completed_jobs
        .<span class="function">iter</span>()
        .<span class="function">take</span>(<span class="number">25_000</span>)
        .<span class="function">cloned</span>()
        .<span class="function">collect</span>();
    <span class="keyword">for</span> id <span class="keyword">in</span> to_remove {
        completed_jobs.<span class="function">remove</span>(&id);
    }
}</code></pre>

      <h3>Job Results TTL</h3>
      <pre><code><span class="comment">// Results are cleaned up based on keepCompletedAge</span>
<span class="keyword">await</span> client.<span class="function">push</span>(<span class="string">'queue'</span>, data, {
  keepCompletedAge: <span class="number">86400000</span>,  <span class="comment">// Keep result for 24h</span>
  keepCompletedCount: <span class="number">100</span>     <span class="comment">// Or keep in last 100</span>
});</code></pre>

      <h3>String Interning</h3>
      <pre><code><span class="comment">// Queue names are interned to reduce allocations</span>
<span class="comment">// Limited to 10K unique names to prevent memory exhaustion</span>
<span class="keyword">static</span> INTERNED: DashMap&lt;String, CompactString&gt; = ...;</code></pre>

      <h2>Background Tasks</h2>

      <p>flashQ runs several background tasks at different intervals:</p>

      <table>
        <thead>
          <tr>
            <th>Task</th>
            <th>Interval</th>
            <th>Purpose</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Wakeup</td>
            <td>100ms</td>
            <td>Notify workers, check dependencies</td>
          </tr>
          <tr>
            <td>Timeout</td>
            <td>500ms</td>
            <td>Check and fail stalled jobs</td>
          </tr>
          <tr>
            <td>Cron</td>
            <td>1s</td>
            <td>Execute scheduled cron jobs</td>
          </tr>
          <tr>
            <td>Metrics</td>
            <td>5s</td>
            <td>Collect metrics history</td>
          </tr>
          <tr>
            <td>Cleanup</td>
            <td>60s</td>
            <td>Clean completed jobs, results, index</td>
          </tr>
        </tbody>
      </table>

      <h2>Multi-Protocol Support</h2>

      <p>flashQ supports four connection methods simultaneously:</p>

      <pre><code><span class="comment"># TCP (default) - lowest latency</span>
PORT=6789 ./flashq-server

<span class="comment"># HTTP REST API + WebSocket</span>
HTTP=1 HTTP_PORT=6790 ./flashq-server

<span class="comment"># gRPC API</span>
GRPC=1 GRPC_PORT=6791 ./flashq-server

<span class="comment"># Unix Socket - highest throughput for local</span>
UNIX_SOCKET=1 ./flashq-server</code></pre>

      <h2>Clustering Architecture</h2>

      <p>For high availability, flashQ supports clustering using PostgreSQL for coordination:</p>

      <pre><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Node 1  â”‚    â”‚  Node 2  â”‚    â”‚  Node 3  â”‚
â”‚ (Leader) â”‚    â”‚(Follower)â”‚    â”‚(Follower)â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚               â”‚               â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
              â”‚  PostgreSQL â”‚
              â”‚  (Shared)   â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</code></pre>

      <p>Leader election uses PostgreSQL advisory locks (<code>pg_try_advisory_lock</code>). Only the leader runs background tasks; all nodes handle client requests.</p>

      <h2>Performance Optimizations Summary</h2>

      <table>
        <thead>
          <tr>
            <th>Optimization</th>
            <th>Benefit</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>DashMap job_index</td>
            <td>Lock-free O(1) lookups, 40% faster</td>
          </tr>
          <tr>
            <td>32 Sharded processing</td>
            <td>-97% contention on ack/fail</td>
          </tr>
          <tr>
            <td>CompactString</td>
            <td>Zero heap alloc for short names</td>
          </tr>
          <tr>
            <td>IndexedPriorityQueue</td>
            <td>O(log n) cancel/update/promote</td>
          </tr>
          <tr>
            <td>MessagePack protocol</td>
            <td>40% smaller, 3-5x faster serialization</td>
          </tr>
          <tr>
            <td>mimalloc allocator</td>
            <td>Faster memory allocation</td>
          </tr>
          <tr>
            <td>parking_lot locks</td>
            <td>Faster than std::sync</td>
          </tr>
          <tr>
            <td>Atomic u64 IDs</td>
            <td>Lock-free ID generation</td>
          </tr>
          <tr>
            <td>Coarse timestamps</td>
            <td>Cached time, fewer syscalls</td>
          </tr>
          <tr>
            <td>LTO build</td>
            <td>Cross-crate optimization</td>
          </tr>
        </tbody>
      </table>

      <h2>Benchmarks</h2>

      <p>On a modern server (32 cores, 64GB RAM):</p>

      <table>
        <thead>
          <tr>
            <th>Operation</th>
            <th>Throughput</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Push (batch)</td>
            <td>1.9M jobs/sec</td>
          </tr>
          <tr>
            <td>Processing (no-op)</td>
            <td>280K jobs/sec</td>
          </tr>
          <tr>
            <td>Processing (CPU work)</td>
            <td>196K jobs/sec</td>
          </tr>
          <tr>
            <td>Concurrent push (10 conn)</td>
            <td>59K ops/sec</td>
          </tr>
        </tbody>
      </table>

      <h2>Conclusion</h2>

      <p>flashQ's architecture is built around one principle: <strong>minimize contention</strong>. By sharding data, using lock-free structures where possible, and optimizing hot paths, we've created a job queue that can handle the most demanding AI workloads.</p>

      <p>The combination of Rust's zero-cost abstractions, careful data structure selection, and attention to memory management allows flashQ to outperform traditional Redis-based solutions while being simpler to operate.</p>

      <div class="article-cta">
        <h3>Try flashQ</h3>
        <p>Experience the performance for yourself.</p>
        <a href="../docs/#quickstart" class="btn btn-primary">Get Started â†’</a>
      </div>
    </div>
  </article>

  <footer>
    <div class="container wide">
      <a href="../" class="logo"><span>âš¡</span> flashQ</a>
      <div class="footer-links">
        <a href="https://github.com/egeominotti/flashq" target="_blank">GitHub</a>
        <a href="https://npmjs.com/package/flashq" target="_blank">npm</a>
        <a href="../docs/">Docs</a>
        <a href="../blog/">Blog</a>
      </div>
      <div class="footer-copy">Â© <span id="year"></span> flashQ. MIT License.</div>
    </div>
  </footer>

  <script>
    const mobileMenuBtn = document.querySelector(".mobile-menu-btn");
    const mobileMenu = document.querySelector(".mobile-menu");
    document.getElementById("year").textContent = new Date().getFullYear();
    mobileMenuBtn.addEventListener("click", () => {
      mobileMenuBtn.classList.toggle("active");
      mobileMenu.classList.toggle("active");
      document.body.style.overflow = mobileMenu.classList.contains("active") ? "hidden" : "";
    });
    mobileMenu.querySelectorAll("a").forEach(link => {
      link.addEventListener("click", () => {
        mobileMenuBtn.classList.remove("active");
        mobileMenu.classList.remove("active");
        document.body.style.overflow = "";
      });
    });
  </script>
</body>
</html>
