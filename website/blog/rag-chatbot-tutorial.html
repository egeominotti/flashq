<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Building a RAG Chatbot with flashQ and OpenAI - flashQ Blog</title>
  <meta name="description" content="Complete tutorial on building a production-ready RAG chatbot using flashQ for job orchestration, OpenAI for embeddings and generation, and Pinecone for vector search.">
  <meta name="keywords" content="rag chatbot, retrieval augmented generation, openai embeddings, pinecone vector search, ai chatbot tutorial, flashq rag">
  <meta name="robots" content="index, follow">
  <meta name="author" content="flashQ Team">

  <meta property="og:title" content="Building a RAG Chatbot with flashQ and OpenAI">
  <meta property="og:description" content="Complete tutorial on building a production-ready RAG chatbot.">
  <meta property="og:type" content="article">
  <meta property="og:url" content="https://flashq.dev/blog/rag-chatbot-tutorial.html">
  <meta property="og:image" content="https://flashq.dev/og-image.png">
  <meta property="og:site_name" content="flashQ">
  <meta property="article:published_time" content="2026-01-06">
  <meta property="article:section" content="Tutorial">
  <meta property="article:tag" content="RAG">
  <meta property="article:tag" content="OpenAI">
  <meta property="article:tag" content="Chatbot">

  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="Building a RAG Chatbot with flashQ and OpenAI">
  <meta name="twitter:description" content="Complete tutorial on building a production-ready RAG chatbot.">
  <meta name="twitter:image" content="https://flashq.dev/og-image.png">

  <link rel="canonical" href="https://flashq.dev/blog/rag-chatbot-tutorial.html">
  <link rel="alternate" type="application/rss+xml" title="flashQ Blog RSS Feed" href="https://flashq.dev/blog/feed.xml">
  <link rel="icon" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='.9em' font-size='90'>âš¡</text></svg>">

  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800;900&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="styles.css">

  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "TechArticle",
    "headline": "Building a RAG Chatbot with flashQ and OpenAI",
    "description": "Complete tutorial on building a production-ready RAG chatbot using flashQ for job orchestration.",
    "datePublished": "2026-01-06",
    "dateModified": "2026-01-06",
    "author": { "@type": "Organization", "name": "flashQ", "url": "https://flashq.dev" },
    "publisher": { "@type": "Organization", "name": "flashQ", "url": "https://flashq.dev" },
    "mainEntityOfPage": { "@type": "WebPage", "@id": "https://flashq.dev/blog/rag-chatbot-tutorial.html" },
    "image": "https://flashq.dev/og-image.png",
    "keywords": ["RAG", "chatbot", "OpenAI", "embeddings", "vector search", "flashQ"]
  }
  </script>

  <!-- Breadcrumb Schema -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "BreadcrumbList",
    "itemListElement": [
      { "@type": "ListItem", "position": 1, "name": "Home", "item": "https://flashq.dev" },
      { "@type": "ListItem", "position": 2, "name": "Blog", "item": "https://flashq.dev/blog/" },
      { "@type": "ListItem", "position": 3, "name": "RAG Chatbot Tutorial", "item": "https://flashq.dev/blog/rag-chatbot-tutorial.html" }
    ]
  }
  </script>
  <!-- Highlight.js -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/typescript.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/bash.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/yaml.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/json.min.js"></script>
</head>
<body>
  <nav>
    <div class="container wide">
      <a href="../" class="logo"><span>âš¡</span> flashQ</a>
      <div class="nav-links">
        <a href="../#features">Features</a>
        <a href="../blog/" class="active">Blog</a>
        <a href="../docs/">Docs</a>
        <a href="https://github.com/egeominotti/flashq" target="_blank">GitHub</a>
        <button class="search-trigger" onclick="openSearch()">
          <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M21 21l-6-6m2-5a7 7 0 11-14 0 7 7 0 0114 0z"/></svg>
          Search <span class="kbd">âŒ˜K</span>
        </button>
        <a href="../docs/#quickstart" class="btn btn-primary">Get Started</a>
      </div>
      <button class="mobile-menu-btn" aria-label="Menu"><span></span><span></span><span></span></button>
    </div>
  </nav>

  <div class="mobile-menu">
    <a href="../#features">Features</a><a href="../blog/">Blog</a><a href="../docs/">Docs</a>
    <a href="https://github.com/egeominotti/flashq" target="_blank">GitHub</a>
    <a href="../docs/#quickstart" class="btn btn-primary">Get Started</a>
  </div>

  <header class="article-header header-ai">
    <div class="container">
      <span class="article-tag tutorial">Tutorial</span>
      <h1>Building a RAG Chatbot with flashQ and OpenAI</h1>
      <div class="article-meta">
        <span>ğŸ“… January 6, 2026</span>
        <span class="reading-time">â±ï¸ 18 min read</span>
      </div>
    </div>
  </header>

  <article class="article-content">
    <div class="container wide">
      <div class="article-layout">
        <div class="article-main">

      <p>Retrieval-Augmented Generation (RAG) is the most powerful pattern for building AI chatbots that can answer questions about your own data. Instead of relying solely on the LLM's training data, RAG retrieves relevant context from your knowledge base before generating a response.</p>

      <p>In this tutorial, we'll build a production-ready RAG chatbot using:</p>
      <ul>
        <li><strong>flashQ</strong> - Job orchestration for the pipeline</li>
        <li><strong>OpenAI</strong> - Embeddings and text generation</li>
        <li><strong>Pinecone</strong> - Vector database for semantic search</li>
      </ul>

      <h2 id="architecture">RAG Architecture</h2>

      <p>A RAG system has two main flows:</p>

      <pre><code class="language-plaintext">INDEXING FLOW (one-time or periodic)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Document â”‚â”€â”€â”€â–¶â”‚  Chunk   â”‚â”€â”€â”€â–¶â”‚  Embed   â”‚â”€â”€â”€â–¶â”‚  Store   â”‚
â”‚  Upload  â”‚    â”‚  Split   â”‚    â”‚  (OpenAI)â”‚    â”‚(Pinecone)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

QUERY FLOW (per user message)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   User   â”‚â”€â”€â”€â–¶â”‚  Embed   â”‚â”€â”€â”€â–¶â”‚  Search  â”‚â”€â”€â”€â–¶â”‚ Generate â”‚
â”‚  Query   â”‚    â”‚  Query   â”‚    â”‚  Context â”‚    â”‚ Response â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</code></pre>

      <p>flashQ orchestrates both flows, handling retries, rate limiting, and progress tracking.</p>

      <h2 id="setup">Project Setup</h2>

      <pre><code class="language-bash"># Initialize project
mkdir rag-chatbot && cd rag-chatbot
npm init -y

# Install dependencies
npm install flashq openai @pinecone-database/pinecone
npm install -D typescript @types/node tsx</code></pre>

      <pre><code class="language-typescript">// .env
FLASHQ_HOST=localhost
FLASHQ_PORT=6789

OPENAI_API_KEY=sk-...
PINECONE_API_KEY=...
PINECONE_INDEX=knowledge-base</code></pre>

      <h2 id="indexing">Document Indexing Pipeline</h2>

      <pre><code class="language-typescript">// src/indexing.ts
import { Queue, Worker } from 'flashq';
import OpenAI from 'openai';
import { Pinecone } from '@pinecone-database/pinecone';

const openai = new OpenAI();
const pinecone = new Pinecone();
const index = pinecone.index(process.env.PINECONE_INDEX!);

const queue = new Queue('rag-indexing');

// Rate limit to stay within OpenAI limits
await queue.setRateLimit(500); // 500 embeddings/minute

// Step 1: Chunk the document
async function chunkDocument(text: string, chunkSize = 500, overlap = 50) {
  const chunks: string[] = [];
  let start = 0;

  while (start < text.length) {
    const end = Math.min(start + chunkSize, text.length);
    chunks.push(text.slice(start, end));
    start += chunkSize - overlap;
  }

  return chunks;
}

// Main indexing function
export async function indexDocument(documentId: string, content: string, metadata: any) {
  // Step 1: Chunk
  const chunkJob = await queue.add('chunk', {
    documentId,
    content,
    metadata,
  });

  // Wait for completion
  const result = await queue.finished(chunkJob.id);
  return result;
}

// Worker for chunking
new Worker('rag-indexing', async (job) => {
  if (job.name === 'chunk') {
    const { documentId, content, metadata } = job.data;

    await job.updateProgress(10, 'Chunking document...');
    const chunks = await chunkDocument(content);

    // Create embedding jobs for each chunk
    const embedJobs = await Promise.all(
      chunks.map((chunk, i) =>
        queue.add('embed', {
          documentId,
          chunkIndex: i,
          text: chunk,
          metadata: { ...metadata, chunkIndex: i },
        })
      )
    );

    // Create store job that depends on all embeddings
    const storeJob = await queue.add('store', {
      documentId,
      totalChunks: chunks.length,
    }, {
      depends_on: embedJobs.map(j => j.id),
    });

    return { storeJobId: storeJob.id, totalChunks: chunks.length };
  }

  if (job.name === 'embed') {
    const { text, documentId, chunkIndex, metadata } = job.data;

    const response = await openai.embeddings.create({
      model: 'text-embedding-3-small',
      input: text,
    });

    return {
      id: `${documentId}-${chunkIndex}`,
      embedding: response.data[0].embedding,
      text,
      metadata,
    };
  }

  if (job.name === 'store') {
    const { documentId, totalChunks } = job.data;

    // Get all embedding results from parent jobs
    const vectors = [];
    for (let i = 0; i < totalChunks; i++) {
      const result = await queue.getResult(`${documentId}-embed-${i}`);
      vectors.push({
        id: result.id,
        values: result.embedding,
        metadata: { ...result.metadata, text: result.text },
      });
    }

    // Upsert to Pinecone in batches
    const batchSize = 100;
    for (let i = 0; i < vectors.length; i += batchSize) {
      await index.upsert(vectors.slice(i, i + batchSize));
    }

    return { indexed: vectors.length, documentId };
  }
});</code></pre>

      <h2 id="query">Query Pipeline</h2>

      <pre><code class="language-typescript">// src/query.ts
import { Queue, Worker } from 'flashq';
import OpenAI from 'openai';
import { Pinecone } from '@pinecone-database/pinecone';

const openai = new OpenAI();
const pinecone = new Pinecone();
const index = pinecone.index(process.env.PINECONE_INDEX!);

const queue = new Queue('rag-query');

// Main query function
export async function askQuestion(question: string, conversationHistory: any[] = []) {
  // Step 1: Embed the question
  const embedJob = await queue.add('embed-query', { question });

  // Step 2: Search (depends on embed)
  const searchJob = await queue.add('search', {
    question,
    topK: 5,
  }, {
    depends_on: [embedJob.id],
  });

  // Step 3: Generate response (depends on search)
  const generateJob = await queue.add('generate', {
    question,
    conversationHistory,
  }, {
    depends_on: [searchJob.id],
  });

  // Wait for final result
  return await queue.finished(generateJob.id);
}

// Query workers
new Worker('rag-query', async (job) => {
  if (job.name === 'embed-query') {
    const { question } = job.data;

    const response = await openai.embeddings.create({
      model: 'text-embedding-3-small',
      input: question,
    });

    return { embedding: response.data[0].embedding };
  }

  if (job.name === 'search') {
    const { question, topK } = job.data;

    // Get embedding from parent job
    const embedResult = await queue.getResult(job.opts.depends_on[0]);

    // Search Pinecone
    const results = await index.query({
      vector: embedResult.embedding,
      topK,
      includeMetadata: true,
    });

    const context = results.matches
      .map(m => m.metadata?.text)
      .filter(Boolean)
      .join('\n\n---\n\n');

    return { context, sources: results.matches };
  }

  if (job.name === 'generate') {
    const { question, conversationHistory } = job.data;

    // Get context from search job
    const searchResult = await queue.getResult(job.opts.depends_on[0]);

    const systemPrompt = `You are a helpful assistant. Answer questions based on the provided context.
If you cannot find the answer in the context, say "I don't have information about that."
Always cite which part of the context you used.

Context:
${searchResult.context}`;

    const messages = [
      { role: 'system', content: systemPrompt },
      ...conversationHistory,
      { role: 'user', content: question },
    ];

    const response = await openai.chat.completions.create({
      model: 'gpt-4',
      messages,
      temperature: 0.7,
      max_tokens: 1000,
    });

    return {
      answer: response.choices[0].message.content,
      sources: searchResult.sources,
      usage: response.usage,
    };
  }
}, {
  concurrency: 10,
});</code></pre>

      <h2 id="api">REST API</h2>

      <pre><code class="language-typescript">// src/server.ts
import express from 'express';
import { indexDocument } from './indexing';
import { askQuestion } from './query';

const app = express();
app.use(express.json({ limit: '10mb' }));

// Index a document
app.post('/api/index', async (req, res) => {
  const { documentId, content, metadata } = req.body;

  const result = await indexDocument(documentId, content, metadata);
  res.json(result);
});

// Ask a question
app.post('/api/chat', async (req, res) => {
  const { question, history } = req.body;

  const result = await askQuestion(question, history || []);
  res.json(result);
});

app.listen(3000, () => {
  console.log('RAG Chatbot API running on port 3000');
});</code></pre>

      <h2 id="usage">Using the Chatbot</h2>

      <pre><code class="language-bash"># Index a document
curl -X POST http://localhost:3000/api/index \
  -H "Content-Type: application/json" \
  -d '{
    "documentId": "doc-1",
    "content": "flashQ is a high-performance job queue...",
    "metadata": { "source": "docs", "title": "flashQ Overview" }
  }'

# Ask a question
curl -X POST http://localhost:3000/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "question": "What is flashQ and how fast is it?"
  }'

# Response
{
  "answer": "flashQ is a high-performance job queue that can process up to 1.9 million jobs per second...",
  "sources": [{ "id": "doc-1-0", "score": 0.92 }],
  "usage": { "prompt_tokens": 450, "completion_tokens": 120 }
}</code></pre>

      <div class="callout callout-success">
        <div class="callout-title">ğŸš€ Production Tips</div>
        <p>1. Use <code>text-embedding-3-large</code> for better accuracy. 2. Implement caching for frequent queries. 3. Add conversation memory with a sliding window. 4. Monitor token usage for cost control.</p>
      </div>

      <h2 id="conclusion">Conclusion</h2>

      <p>You've built a production-ready RAG chatbot with:</p>
      <ul>
        <li><strong>Document indexing</strong> with chunking and embeddings</li>
        <li><strong>Semantic search</strong> using Pinecone</li>
        <li><strong>Context-aware generation</strong> with GPT-4</li>
        <li><strong>Job orchestration</strong> with flashQ for reliability</li>
      </ul>

      <p>flashQ handles the complex orchestrationâ€”retries, rate limiting, progress trackingâ€”so you can focus on building great AI experiences.</p>

      <div class="article-cta">
        <h3>Build Your Own RAG Chatbot</h3>
        <p>Get started with flashQ and build intelligent AI applications.</p>
        <a href="../docs/#quickstart" class="btn btn-primary">Get Started â†’</a>
      </div>
        </div>

        <aside class="toc-sidebar">
          <nav class="toc">
            <div class="toc-title">On this page</div>
            <ul class="toc-list">
              <li><a href="#architecture">RAG Architecture</a></li>
              <li><a href="#setup">Project Setup</a></li>
              <li><a href="#indexing">Indexing Pipeline</a></li>
              <li><a href="#query">Query Pipeline</a></li>
              <li><a href="#api">REST API</a></li>
              <li><a href="#usage">Using the Chatbot</a></li>
              <li><a href="#conclusion">Conclusion</a></li>
            </ul>
          </nav>
        </aside>
      </div>
    </div>
  </article>

  <footer>
    <div class="container wide">
      <a href="../" class="logo"><span>âš¡</span> flashQ</a>
      <div class="footer-links">
        <a href="https://github.com/egeominotti/flashq" target="_blank">GitHub</a>
        <a href="https://npmjs.com/package/flashq" target="_blank">npm</a>
        <a href="../docs/">Docs</a>
        <a href="../blog/">Blog</a>
      </div>
      <div class="footer-copy">Â© <span id="year"></span> flashQ. MIT License.</div>
    </div>
  </footer>

  <div class="search-overlay" id="searchOverlay" onclick="closeSearch(event)">
    <div class="search-modal" onclick="event.stopPropagation()">
      <div class="search-input-wrapper">
        <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M21 21l-6-6m2-5a7 7 0 11-14 0 7 7 0 0114 0z"/></svg>
        <input type="text" class="search-modal-input" id="searchInput" placeholder="Search...">
        <span class="search-shortcut">ESC</span>
      </div>
      <div class="search-results" id="searchResults"></div>
    </div>
  </div>

  <script>
    document.getElementById("year").textContent = new Date().getFullYear();
    const mobileMenuBtn = document.querySelector(".mobile-menu-btn");
    const mobileMenu = document.querySelector(".mobile-menu");
    mobileMenuBtn.addEventListener("click", () => { mobileMenuBtn.classList.toggle("active"); mobileMenu.classList.toggle("active"); });
    const tocLinks = document.querySelectorAll('.toc-list a');
    const headings = document.querySelectorAll('h2[id]');
    function updateTocActive() { let current = ''; headings.forEach(h => { if (window.scrollY >= h.offsetTop - 120) current = h.id; }); tocLinks.forEach(l => l.classList.toggle('active', l.getAttribute('href') === '#' + current)); }
    window.addEventListener('scroll', updateTocActive); updateTocActive();
    function openSearch() { document.getElementById('searchOverlay').classList.add('active'); document.getElementById('searchInput').focus(); }
    function closeSearch(e) { if (e?.target === document.getElementById('searchOverlay')) document.getElementById('searchOverlay').classList.remove('active'); }
    document.addEventListener('keydown', e => { if ((e.metaKey||e.ctrlKey) && e.key==='k') { e.preventDefault(); openSearch(); } if (e.key==='Escape') closeSearch({target:document.getElementById('searchOverlay')}); });
  </script>
  <script>hljs.highlightAll();</script>
</body>
</html>
