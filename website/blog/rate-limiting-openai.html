<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>How to Rate Limit OpenAI API Calls with flashQ - flashQ Blog</title>
  <meta name="description" content="Learn how to properly rate limit OpenAI, Anthropic, and other AI API calls using flashQ. Avoid 429 errors and control costs.">
  <meta name="keywords" content="openai rate limit, api rate limiting, 429 error, openai quota, anthropic rate limit, ai api costs">
  <meta name="robots" content="index, follow">

  <meta property="og:title" content="How to Rate Limit OpenAI API Calls with flashQ">
  <meta property="og:description" content="Learn how to properly rate limit OpenAI and other AI API calls.">
  <meta property="og:type" content="article">
  <meta property="og:url" content="https://flashq.dev/blog/rate-limiting-openai.html">
  <meta property="og:image" content="https://flashq.dev/og-image.png">

  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="How to Rate Limit OpenAI API Calls with flashQ">
  <meta name="twitter:image" content="https://flashq.dev/og-image.png">

  <meta property="article:published_time" content="2025-12-04">

  <link rel="canonical" href="https://flashq.dev/blog/rate-limiting-openai.html">
  <link rel="icon" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='.9em' font-size='90'>‚ö°</text></svg>">

  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800;900&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">

  <link rel="stylesheet" href="styles.css">

  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Article",
    "headline": "How to Rate Limit OpenAI API Calls with flashQ",
    "datePublished": "2025-12-04",
    "author": { "@type": "Organization", "name": "flashQ" }
  }
  </script>

  <!-- Breadcrumb Schema -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "BreadcrumbList",
    "itemListElement": [
      { "@type": "ListItem", "position": 1, "name": "Home", "item": "https://flashq.dev" },
      { "@type": "ListItem", "position": 2, "name": "Blog", "item": "https://flashq.dev/blog/" },
      { "@type": "ListItem", "position": 3, "name": "Rate Limiting OpenAI", "item": "https://flashq.dev/blog/rate-limiting-openai.html" }
    ]
  }
  </script>
  <!-- Highlight.js -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/typescript.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/bash.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/yaml.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/json.min.js"></script>
</head>
<body>
  <nav>
    <div class="container wide">
      <a href="../" class="logo"><span>‚ö°</span> flashQ</a>
      <div class="nav-links">
        <a href="../#features">Features</a>
        <a href="../blog/" class="active">Blog</a>
        <a href="../docs/">Docs</a>
        <a href="https://github.com/egeominotti/flashq" target="_blank">GitHub</a>
        <a href="../docs/#quickstart" class="btn btn-primary">Get Started</a>
      </div>
      <button class="mobile-menu-btn" aria-label="Menu">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
  </nav>

  <div class="mobile-menu">
    <a href="../#features">Features</a>
    <a href="../blog/">Blog</a>
    <a href="../docs/">Docs</a>
    <a href="https://github.com/egeominotti/flashq" target="_blank">GitHub</a>
    <a href="../docs/#quickstart" class="btn btn-primary">Get Started</a>
  </div>

  <header class="article-header header-ai">
    <div class="container">
      <span class="article-tag tutorial">Tutorial</span>
      <h1>How to Rate Limit OpenAI API Calls with flashQ</h1>
      <div class="article-meta">
        <span>üìÖ December 4, 2025</span>
        <span>‚è±Ô∏è 8 min read</span>
      </div>
    </div>
  </header>

  <article class="article-content">
    <div class="container">
      <p>If you've worked with OpenAI's API, you've probably seen this error:</p>

      <pre><code class="language-typescript">Error 429: Rate limit reached for gpt-4 in organization org-xxx
on requests per min (RPM): Limit 500, Used 500, Requested 1.</code></pre>

      <p>Rate limits are a fact of life when working with AI APIs. OpenAI, Anthropic, Cohere, and every other provider enforces them. Without proper handling, your application will crash, users will see errors, and you'll waste money on failed requests.</p>

      <p>In this tutorial, we'll build a robust rate-limiting system using flashQ that:</p>

      <ul>
        <li>Never exceeds API rate limits</li>
        <li>Automatically retries failed requests</li>
        <li>Tracks costs per request</li>
        <li>Handles multiple API tiers</li>
      </ul>

      <h2>Understanding OpenAI Rate Limits</h2>

      <p>OpenAI has two types of rate limits:</p>

      <table>
        <thead>
          <tr>
            <th>Limit Type</th>
            <th>Description</th>
            <th>Typical Values (Tier 1)</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><strong>RPM</strong></td>
            <td>Requests per minute</td>
            <td>500 RPM for GPT-4</td>
          </tr>
          <tr>
            <td><strong>TPM</strong></td>
            <td>Tokens per minute</td>
            <td>10,000 TPM for GPT-4</td>
          </tr>
        </tbody>
      </table>

      <p>Your tier depends on how much you've spent. New accounts start at Tier 1 with lower limits.</p>

      <h2>Basic Rate Limiting with flashQ</h2>

      <p>flashQ has built-in token bucket rate limiting. Here's the simplest setup:</p>

      <pre><code class="language-typescript">import { Queue, Worker } from 'flashq';
import OpenAI from 'openai';

const openai = new OpenAI();
const queue = new Queue('openai-calls');

// Set rate limit: 400 requests per minute (leaving buffer)
await queue.setRateLimit(400);

// Add jobs
await queue.add('chat', {
  model: 'gpt-4',
  messages: [{ role: 'user', content: 'Hello!' }]
});

// Worker processes at controlled rate
new Worker('openai-calls', async (job) => {
  const response = await openai.chat.completions.create(job.data);
  return response.choices[0].message.content;
});</code></pre>

      <p>The queue will now process at most 400 jobs per minute, regardless of how many jobs you add.</p>

      <h2>Handling 429 Errors with Retries</h2>

      <p>Even with rate limiting, you might still hit 429 errors during traffic spikes. Configure automatic retries:</p>

      <pre><code class="language-typescript">// Add job with retry configuration
await queue.add('chat', {
  model: 'gpt-4',
  messages: [{ role: 'user', content: 'Explain quantum computing' }]
}, {
  attempts: 5,
  backoff: {
    type: 'exponential',
    delay: 2000  // 2s, 4s, 8s, 16s, 32s
  }
});

// Worker with error handling
new Worker('openai-calls', async (job) => {
  try {
    const response = await openai.chat.completions.create(job.data);
    return {
      content: response.choices[0].message.content,
      usage: response.usage
    };
  } catch (error) {
    if (error.status === 429) {
      // Rate limited - throw to trigger retry
      throw new Error('Rate limited by OpenAI');
    }
    if (error.status === 400) {
      // Bad request - don't retry, return error
      return { error: error.message };
    }
    throw error; // Other errors - retry
  }
});</code></pre>

      <h2>Different Queues for Different Models</h2>

      <p>Each OpenAI model has different rate limits. Create separate queues:</p>

      <pre><code class="language-typescript">// GPT-4: Lower limits, higher cost
const gpt4Queue = new Queue('openai-gpt4');
await gpt4Queue.setRateLimit(400); // 400 RPM

// GPT-3.5: Higher limits, lower cost
const gpt35Queue = new Queue('openai-gpt35');
await gpt35Queue.setRateLimit(3000); // 3000 RPM

// Embeddings: Very high limits
const embeddingsQueue = new Queue('openai-embeddings');
await embeddingsQueue.setRateLimit(5000); // 5000 RPM

// Route requests to appropriate queue
function getQueue(model) {
  if (model.startsWith('gpt-4')) return gpt4Queue;
  if (model.startsWith('gpt-3.5')) return gpt35Queue;
  if (model.includes('embedding')) return embeddingsQueue;
  return gpt35Queue;
}</code></pre>

      <h2>Tracking Costs</h2>

      <p>AI API costs can spiral quickly. Track them per job:</p>

      <pre><code class="language-typescript">// Pricing per 1K tokens (as of 2024)
const PRICING = {
  'gpt-4': { input: 0.03, output: 0.06 },
  'gpt-4-turbo': { input: 0.01, output: 0.03 },
  'gpt-3.5-turbo': { input: 0.0005, output: 0.0015 },
  'text-embedding-3-small': { input: 0.00002, output: 0 }
};

function calculateCost(model, usage) {
  const prices = PRICING[model] || PRICING['gpt-3.5-turbo'];
  const inputCost = (usage.prompt_tokens / 1000) * prices.input;
  const outputCost = (usage.completion_tokens / 1000) * prices.output;
  return inputCost + outputCost;
}

// Worker with cost tracking
new Worker('openai-gpt4', async (job) => {
  const response = await openai.chat.completions.create(job.data);
  const cost = calculateCost(job.data.model, response.usage);

  return {
    content: response.choices[0].message.content,
    usage: response.usage,
    cost: cost
  };
});

// Aggregate costs
let totalCost = 0;
gpt4Queue.on('completed', (job, result) => {
  totalCost += result.cost;
  console.log(`Job cost: $${result.cost.toFixed(4)}, Total: $${totalCost.toFixed(2)}`);
});</code></pre>

      <h2>Budget Controls</h2>

      <p>Stop processing when you hit a budget limit:</p>

      <pre><code class="language-typescript">const DAILY_BUDGET = 100; // $100/day
let dailySpend = 0;

gpt4Queue.on('completed', async (job, result) => {
  dailySpend += result.cost;

  if (dailySpend >= DAILY_BUDGET) {
    console.log('Daily budget reached! Pausing queue.');
    await gpt4Queue.pause();

    // Alert team
    await sendSlackAlert(`OpenAI daily budget of $${DAILY_BUDGET} reached`);
  }
});

// Reset daily spend at midnight
setInterval(async () => {
  const now = new Date();
  if (now.getHours() === 0 && now.getMinutes() === 0) {
    dailySpend = 0;
    await gpt4Queue.resume();
    console.log('Daily budget reset. Queue resumed.');
  }
}, 60000); // Check every minute</code></pre>

      <h2>Handling Multiple Providers</h2>

      <p>Most teams use multiple AI providers. Here's a pattern for that:</p>

      <pre><code class="language-typescript">import OpenAI from 'openai';
import Anthropic from '@anthropic-ai/sdk';

const openai = new OpenAI();
const anthropic = new Anthropic();

// Separate queues per provider
const openaiQueue = new Queue('llm-openai');
const anthropicQueue = new Queue('llm-anthropic');

await openaiQueue.setRateLimit(500);
await anthropicQueue.setRateLimit(1000);

// Unified interface
async function chat(provider, messages, options = {}) {
  const queue = provider === 'anthropic' ? anthropicQueue : openaiQueue;

  const job = await queue.add('chat', {
    provider,
    messages,
    ...options
  });

  return queue.finished(job.id);
}

// Workers
new Worker('llm-openai', async (job) => {
  const response = await openai.chat.completions.create({
    model: job.data.model || 'gpt-4',
    messages: job.data.messages
  });
  return response.choices[0].message.content;
});

new Worker('llm-anthropic', async (job) => {
  const response = await anthropic.messages.create({
    model: job.data.model || 'claude-3-opus-20240229',
    max_tokens: 1024,
    messages: job.data.messages
  });
  return response.content[0].text;
});</code></pre>

      <h2>Production Checklist</h2>

      <p>Before going to production with rate-limited AI calls:</p>

      <ul>
        <li>‚úÖ Set rate limits below your API tier limits (leave 10-20% buffer)</li>
        <li>‚úÖ Configure exponential backoff for retries</li>
        <li>‚úÖ Track costs per job and set budget alerts</li>
        <li>‚úÖ Use separate queues for different models/providers</li>
        <li>‚úÖ Monitor the dead letter queue for persistent failures</li>
        <li>‚úÖ Set up alerts for high error rates</li>
        <li>‚úÖ Test failover behavior when limits are hit</li>
      </ul>

      <div class="callout callout-success">
        <div class="callout-title">üí° Pro Tip</div>
        <p>Request a rate limit increase from OpenAI once you have consistent usage. They're generally responsive and will bump your limits based on spend history.</p>
      </div>

      <h2>Conclusion</h2>

      <p>Rate limiting AI API calls is essential for building reliable applications. With flashQ's built-in rate limiting, automatic retries, and event system, you can build robust AI applications that never exceed quotas and stay within budget.</p>

      <div class="article-cta">
        <h3>Start Building</h3>
        <p>Get flashQ running and start rate limiting your AI calls in minutes.</p>
        <a href="../docs/#rate-limiting" class="btn btn-primary">Read the Docs ‚Üí</a>
      </div>
    </div>
  </article>

  <footer>
    <div class="container wide">
      <a href="../" class="logo"><span>‚ö°</span> flashQ</a>
      <div class="footer-links">
        <a href="https://github.com/egeominotti/flashq" target="_blank">GitHub</a>
        <a href="https://npmjs.com/package/flashq" target="_blank">npm</a>
        <a href="../docs/">Docs</a>
        <a href="../blog/">Blog</a>
      </div>
      <div class="footer-copy">¬© <span id="year"></span> flashQ. MIT License.</div>
    </div>
  </footer>
  <script>
    const mobileMenuBtn = document.querySelector(".mobile-menu-btn");
    const mobileMenu = document.querySelector(".mobile-menu");
    document.getElementById("year").textContent = new Date().getFullYear();
    mobileMenuBtn.addEventListener("click", () => {
      mobileMenuBtn.classList.toggle("active");
      mobileMenu.classList.toggle("active");
      document.body.style.overflow = mobileMenu.classList.contains("active") ? "hidden" : "";
    });
    mobileMenu.querySelectorAll("a").forEach(link => {
      link.addEventListener("click", () => {
        mobileMenuBtn.classList.remove("active");
        mobileMenu.classList.remove("active");
        document.body.style.overflow = "";
      });
    });
  </script>
  <script>hljs.highlightAll();</script>
</body>
</html>
