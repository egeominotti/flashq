<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>What is flashQ? A Modern Job Queue for AI Workloads - flashQ Blog</title>
  <meta name="description" content="Discover flashQ, a high-performance job queue designed for AI and ML workloads. BullMQ-compatible API, 10x faster, no Redis required.">
  <meta name="keywords" content="flashq, job queue, ai workloads, background jobs, redis alternative, bullmq">
  <meta name="robots" content="index, follow">

  <!-- Open Graph -->
  <meta property="og:title" content="What is flashQ? A Modern Job Queue for AI Workloads">
  <meta property="og:description" content="Discover flashQ, a high-performance job queue designed for AI and ML workloads.">
  <meta property="og:type" content="article">
  <meta property="og:url" content="https://flashq.dev/blog/what-is-flashq.html">
  <meta property="og:image" content="https://flashq.dev/og-image.png">
  <meta property="article:published_time" content="2025-11-19">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="What is flashQ? A Modern Job Queue for AI Workloads">
  <meta name="twitter:description" content="Discover flashQ, a high-performance job queue designed for AI and ML workloads.">
  <meta name="twitter:image" content="https://flashq.dev/og-image.png">

  <link rel="canonical" href="https://flashq.dev/blog/what-is-flashq.html">
  <link rel="icon" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='.9em' font-size='90'>âš¡</text></svg>">

  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800;900&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">

  <link rel="stylesheet" href="styles.css">

  <!-- Article Schema -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Article",
    "headline": "What is flashQ? A Modern Job Queue for AI Workloads",
    "description": "Discover flashQ, a high-performance job queue designed for AI and ML workloads.",
    "datePublished": "2025-11-19",
    "author": {
      "@type": "Organization",
      "name": "flashQ"
    },
    "publisher": {
      "@type": "Organization",
      "name": "flashQ",
      "url": "https://flashq.dev"
    }
  }
  </script>

  <!-- Breadcrumb Schema -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "BreadcrumbList",
    "itemListElement": [
      { "@type": "ListItem", "position": 1, "name": "Home", "item": "https://flashq.dev" },
      { "@type": "ListItem", "position": 2, "name": "Blog", "item": "https://flashq.dev/blog/" },
      { "@type": "ListItem", "position": 3, "name": "What is flashQ?", "item": "https://flashq.dev/blog/what-is-flashq.html" }
    ]
  }
  </script>
  <!-- Highlight.js -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/typescript.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/bash.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/yaml.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/json.min.js"></script>
</head>
<body>
  <!-- Navigation -->
  <nav>
    <div class="container wide">
      <a href="../" class="logo">
        <span>âš¡</span> flashQ
      </a>
      <div class="nav-links">
        <a href="../#features">Features</a>
        <a href="../blog/" class="active">Blog</a>
        <a href="../docs/">Docs</a>
        <a href="https://github.com/egeominotti/flashq" target="_blank">GitHub</a>
        <a href="../docs/#quickstart" class="btn btn-primary">Get Started</a>
      </div>
      <button class="mobile-menu-btn" aria-label="Menu">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
  </nav>

  <div class="mobile-menu">
    <a href="../#features">Features</a>
    <a href="../blog/">Blog</a>
    <a href="../docs/">Docs</a>
    <a href="https://github.com/egeominotti/flashq" target="_blank">GitHub</a>
    <a href="../docs/#quickstart" class="btn btn-primary">Get Started</a>
  </div>

  <!-- Article Header -->
  <header class="article-header header-tutorial">
    <div class="container">
      <span class="article-tag">Introduction</span>
      <h1>What is flashQ? A Modern Job Queue for AI Workloads</h1>
      <div class="article-meta">
        <span>ğŸ“… November 19, 2025</span>
        <span>â±ï¸ 8 min read</span>
      </div>
    </div>
  </header>

  <!-- Article Content -->
  <article class="article-content">
    <div class="container">
      <p>If you've ever built an application that needs to process tasks in the backgroundâ€”sending emails, generating reports, processing images, or calling AI APIsâ€”you've probably encountered job queues. They're the backbone of scalable applications, allowing you to offload work from your main application thread and process it asynchronously.</p>

      <p><strong>flashQ</strong> is a high-performance job queue built specifically for modern AI workloads. It's designed to be fast, simple, and reliableâ€”without requiring you to manage Redis or any external infrastructure.</p>

      <h2>The Problem with Traditional Job Queues</h2>

      <p>Most job queues in the Node.js ecosystem rely on Redis. Tools like BullMQ, Bull, and Bee-Queue are excellent, but they come with a significant operational overhead:</p>

      <ul>
        <li><strong>Redis management</strong>: You need to provision, configure, and maintain a Redis instance</li>
        <li><strong>Memory costs</strong>: Redis stores everything in memory, which gets expensive at scale</li>
        <li><strong>Persistence concerns</strong>: Redis persistence (RDB/AOF) requires careful tuning</li>
        <li><strong>Network latency</strong>: Every job operation requires a network round-trip to Redis</li>
        <li><strong>Payload limitations</strong>: Redis has practical limits on value sizes (~512MB, but performance degrades much earlier)</li>
      </ul>

      <p>For AI workloads specifically, these limitations become even more painful. AI applications often need to:</p>

      <ul>
        <li>Send large payloads (embeddings, images, long text contexts)</li>
        <li>Chain multiple operations together (embed â†’ search â†’ generate)</li>
        <li>Rate limit API calls to avoid hitting provider quotas</li>
        <li>Handle long-running jobs (minutes, not seconds)</li>
      </ul>

      <h2>Enter flashQ</h2>

      <p>flashQ was built from the ground up to solve these problems. Here's what makes it different:</p>

      <h3>1. No Redis Required</h3>

      <p>flashQ is a standalone server written in Rust. You run a single binary, and you're done. No Redis to provision, no connection strings to manage, no memory to monitor.</p>

      <pre><code class="language-bash"># Start flashQ server
./flashq-server

# Or with Docker
docker run -p 6789:6789 flashq/flashq</code></pre>

      <p>For persistence, flashQ can optionally connect to PostgreSQL. But for many use cases, the in-memory mode is perfectly sufficient.</p>

      <h3>2. BullMQ-Compatible API</h3>

      <p>If you're already using BullMQ, switching to flashQ is trivial. The API is intentionally compatible:</p>

      <pre><code class="language-typescript">// Before (BullMQ)
import { Queue, Worker } from 'bullmq';

// After (flashQ)
import { Queue, Worker } from 'flashq';</code></pre>

      <p>That's it. Your existing code works with minimal changes.</p>

      <h3>3. 10x Faster</h3>

      <p>Because flashQ eliminates the network hop to Redis and is written in Rust with careful attention to performance, it's significantly faster:</p>

      <table>
        <thead>
          <tr>
            <th>Metric</th>
            <th>flashQ</th>
            <th>BullMQ + Redis</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Push throughput</td>
            <td><strong>1.9M jobs/sec</strong></td>
            <td>~50K jobs/sec</td>
          </tr>
          <tr>
            <td>Processing throughput</td>
            <td><strong>280K jobs/sec</strong></td>
            <td>~30K jobs/sec</td>
          </tr>
          <tr>
            <td>Latency (p99)</td>
            <td><strong>&lt;1ms</strong></td>
            <td>~5-10ms</td>
          </tr>
        </tbody>
      </table>

      <h3>4. Built for AI Workloads</h3>

      <p>flashQ has features specifically designed for AI applications:</p>

      <ul>
        <li><strong>10MB payload limit</strong>: Send embeddings, images, and large contexts without workarounds</li>
        <li><strong>Job dependencies</strong>: Chain jobs with <code>depends_on</code> for RAG pipelines</li>
        <li><strong>Rate limiting</strong>: Built-in token bucket rate limiting per queue</li>
        <li><strong>Long timeouts</strong>: Jobs can run for minutes without being marked as stalled</li>
        <li><strong>Progress tracking</strong>: Report progress for long-running inference jobs</li>
      </ul>

      <h2>How flashQ Works</h2>

      <p>At its core, flashQ is a TCP server that accepts commands and manages job queues. Here's the architecture:</p>

      <pre><code class="language-plaintext">â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Your App      â”‚     â”‚   Workers       â”‚
â”‚  (Producer)     â”‚     â”‚  (Consumers)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚
         â”‚    TCP/HTTP/gRPC      â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
              â”‚   flashQ    â”‚
              â”‚   Server    â”‚
              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
              â”‚  PostgreSQL â”‚ (optional)
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</code></pre>

      <p>The server maintains queues in memory using efficient data structures:</p>

      <ul>
        <li><strong>32 shards</strong> for parallel access without lock contention</li>
        <li><strong>Priority queues</strong> (binary heaps) for job ordering</li>
        <li><strong>Hash maps</strong> for O(1) job lookups</li>
        <li><strong>Atomic counters</strong> for job IDs and metrics</li>
      </ul>

      <h2>A Simple Example</h2>

      <p>Let's build a simple AI pipeline that generates embeddings and stores them:</p>

      <pre><code class="language-typescript">import { Queue, Worker } from 'flashq';

// Create a queue
const queue = new Queue('embeddings');

// Add a job
await queue.add('generate', {
  text: 'The quick brown fox jumps over the lazy dog',
  model: 'text-embedding-3-small'
});

// Process jobs
const worker = new Worker('embeddings', async (job) => {
  const { text, model } = job.data;

  // Call OpenAI API
  const response = await openai.embeddings.create({
    input: text,
    model: model
  });

  // Return the embedding
  return response.data[0].embedding;
});</code></pre>

      <h2>When Should You Use flashQ?</h2>

      <p>flashQ is ideal for:</p>

      <ul>
        <li><strong>AI/ML pipelines</strong>: LLM calls, embeddings, image generation, batch inference</li>
        <li><strong>Startups and small teams</strong>: No infrastructure to manage</li>
        <li><strong>High-throughput applications</strong>: When you need more than 30K jobs/sec</li>
        <li><strong>Large payloads</strong>: When your job data exceeds a few KB</li>
        <li><strong>Development environments</strong>: No Docker Compose file just to run Redis</li>
      </ul>

      <p>You might prefer BullMQ + Redis if:</p>

      <ul>
        <li>You're already running Redis for other purposes (caching, sessions)</li>
        <li>You need Redis-specific features (pub/sub, streams)</li>
        <li>Your team has deep Redis expertise</li>
      </ul>

      <h2>Getting Started</h2>

      <p>Ready to try flashQ? It takes about 5 minutes to get started:</p>

      <pre><code class="language-bash"># Install the SDK
npm install flashq

# Start the server (using Docker)
docker run -d -p 6789:6789 flashq/flashq

# Or download the binary
curl -L https://github.com/egeominotti/flashq/releases/latest/download/flashq-linux -o flashq
chmod +x flashq
./flashq</code></pre>

      <p>Then in your code:</p>

      <pre><code class="language-typescript">import { Queue, Worker } from 'flashq';

const queue = new Queue('my-queue');
await queue.add('task', { hello: 'world' });

const worker = new Worker('my-queue', async (job) => {
  console.log(job.data); // { hello: 'world' }
});</code></pre>

      <div class="callout callout-success">
        <div class="callout-title">ğŸ’¡ Pro Tip</div>
        <p>Check out the <a href="../docs/">documentation</a> for advanced features like job dependencies, rate limiting, and clustering.</p>
      </div>

      <h2>Conclusion</h2>

      <p>flashQ represents a new approach to job queuesâ€”one that prioritizes simplicity and performance without sacrificing features. If you're building AI applications and tired of managing Redis, give flashQ a try.</p>

      <p>We're open source and actively developing new features. Join us on <a href="https://github.com/egeominotti/flashq">GitHub</a> and let us know what you think!</p>

      <!-- CTA -->
      <div class="article-cta">
        <h3>Ready to try flashQ?</h3>
        <p>Get started in 5 minutes with our quickstart guide.</p>
        <a href="../docs/#quickstart" class="btn btn-primary">Get Started â†’</a>
      </div>
    </div>
  </article>

  <!-- Footer -->
  <footer>
    <div class="container wide">
      <a href="../" class="logo">
        <span>âš¡</span> flashQ
      </a>
      <div class="footer-links">
        <a href="https://github.com/egeominotti/flashq" target="_blank">GitHub</a>
        <a href="https://npmjs.com/package/flashq" target="_blank">npm</a>
        <a href="../docs/">Docs</a>
        <a href="../blog/">Blog</a>
      </div>
      <div class="footer-copy">
        Â© <span id="year"></span> flashQ. MIT License.
      </div>
    </div>
  </footer>
  <script>
    const mobileMenuBtn = document.querySelector(".mobile-menu-btn");
    const mobileMenu = document.querySelector(".mobile-menu");
    document.getElementById("year").textContent = new Date().getFullYear();
    mobileMenuBtn.addEventListener("click", () => {
      mobileMenuBtn.classList.toggle("active");
      mobileMenu.classList.toggle("active");
      document.body.style.overflow = mobileMenu.classList.contains("active") ? "hidden" : "";
    });
    mobileMenu.querySelectorAll("a").forEach(link => {
      link.addEventListener("click", () => {
        mobileMenuBtn.classList.remove("active");
        mobileMenu.classList.remove("active");
        document.body.style.overflow = "";
      });
    });
  </script>
  <script>hljs.highlightAll();</script>
</body>
</html>
