<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Building AI Pipelines with flashQ: A Complete Guide - flashQ Blog</title>
  <meta name="description" content="Learn how to build production-ready AI pipelines with flashQ. RAG workflows, LLM orchestration, batch inference, and more.">
  <meta name="keywords" content="ai pipeline, rag workflow, llm orchestration, job queue ai, flashq tutorial, batch inference">
  <meta name="robots" content="index, follow">

  <!-- Open Graph -->
  <meta property="og:title" content="Building AI Pipelines with flashQ: A Complete Guide">
  <meta property="og:description" content="Learn how to build production-ready AI pipelines with flashQ.">
  <meta property="og:type" content="article">
  <meta property="og:url" content="https://flashq.dev/blog/building-ai-pipelines.html">
  <meta property="og:image" content="https://flashq.dev/og-image.png">
  <meta property="article:published_time" content="2025-11-28">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="Building AI Pipelines with flashQ: A Complete Guide">
  <meta name="twitter:description" content="Learn how to build production-ready AI pipelines with flashQ.">
  <meta name="twitter:image" content="https://flashq.dev/og-image.png">

  <link rel="canonical" href="https://flashq.dev/blog/building-ai-pipelines.html">
  <link rel="alternate" type="application/rss+xml" title="flashQ Blog RSS Feed" href="https://flashq.dev/blog/feed.xml">
  <link rel="icon" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='.9em' font-size='90'>âš¡</text></svg>">

  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800;900&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">

  <link rel="stylesheet" href="styles.css">

  <!-- Article Schema -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Article",
    "headline": "Building AI Pipelines with flashQ: A Complete Guide",
    "description": "Learn how to build production-ready AI pipelines with flashQ.",
    "datePublished": "2025-11-28",
    "author": {
      "@type": "Organization",
      "name": "flashQ"
    }
  }
  </script>

  <!-- Breadcrumb Schema -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "BreadcrumbList",
    "itemListElement": [
      { "@type": "ListItem", "position": 1, "name": "Home", "item": "https://flashq.dev" },
      { "@type": "ListItem", "position": 2, "name": "Blog", "item": "https://flashq.dev/blog/" },
      { "@type": "ListItem", "position": 3, "name": "Building AI Pipelines", "item": "https://flashq.dev/blog/building-ai-pipelines.html" }
    ]
  }
  </script>
  <!-- Highlight.js -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/typescript.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/bash.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/yaml.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/json.min.js"></script>
</head>
<body>
  <!-- Navigation -->
  <nav>
    <div class="container wide">
      <a href="../" class="logo">
        <span>âš¡</span> flashQ
      </a>
      <div class="nav-links">
        <a href="../#features">Features</a>
        <a href="../blog/" class="active">Blog</a>
        <a href="../docs/">Docs</a>
        <a href="https://github.com/egeominotti/flashq" target="_blank">GitHub</a>
        <button class="search-trigger" onclick="openSearch()">
          <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M21 21l-6-6m2-5a7 7 0 11-14 0 7 7 0 0114 0z"/></svg>
          Search
          <span class="kbd">âŒ˜K</span>
        </button>
        <a href="../docs/#quickstart" class="btn btn-primary">Get Started</a>
      </div>
      <button class="mobile-menu-btn" aria-label="Menu">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
  </nav>

  <div class="mobile-menu">
    <a href="../#features">Features</a>
    <a href="../blog/">Blog</a>
    <a href="../docs/">Docs</a>
    <a href="https://github.com/egeominotti/flashq" target="_blank">GitHub</a>
    <a href="../docs/#quickstart" class="btn btn-primary">Get Started</a>
  </div>

  <!-- Breadcrumb -->
  <!-- Article Header -->
  <header class="article-header header-ai">
    <div class="container">
      <span class="article-tag tutorial">Tutorial</span>
      <h1>Building AI Pipelines with flashQ: A Complete Guide</h1>
      <div class="article-meta">
        <span>ğŸ“… November 28, 2025</span>
        <span class="reading-time">â±ï¸ 15 min read</span>
      </div>
    </div>
  </header>

  <!-- Article Content -->
  <article class="article-content">
    <div class="container wide">
      <div class="article-layout">
        <div class="article-main">
      <p>AI applications are rarely simple request-response systems. They involve complex pipelines: generating embeddings, searching vector databases, calling LLMs, post-processing results, and more. Each step can fail, needs retry logic, and often takes seconds to complete.</p>

      <p>This is where job queues shine. In this guide, we'll build three real-world AI pipelines using flashQ:</p>

      <ol>
        <li><strong>RAG Pipeline</strong>: Embed â†’ Search â†’ Generate</li>
        <li><strong>Document Processing</strong>: Parse â†’ Chunk â†’ Embed â†’ Store</li>
        <li><strong>Batch Inference</strong>: Process millions of predictions</li>
      </ol>

      <h2 id="why-job-queue">Why Use a Job Queue for AI?</h2>

      <p>Before diving into code, let's understand why job queues are essential for AI applications:</p>

      <ul>
        <li><strong>Rate Limiting</strong>: OpenAI, Anthropic, and other providers have strict rate limits. A queue lets you control throughput.</li>
        <li><strong>Reliability</strong>: API calls fail. Queues provide automatic retries with exponential backoff.</li>
        <li><strong>Cost Control</strong>: Track costs per job, pause queues when budgets are exceeded.</li>
        <li><strong>Long-Running Jobs</strong>: LLM calls can take 30+ seconds. Don't block your web server.</li>
        <li><strong>Orchestration</strong>: Chain multiple steps with dependencies.</li>
      </ul>

      <h2 id="setup">Setup</h2>

      <p>First, let's set up flashQ:</p>

      <pre><code class="language-bash"># Install flashQ SDK
npm install flashq

# Start the server
docker run -d -p 6789:6789 flashq/flashq</code></pre>

      <p>And install the AI SDKs we'll use:</p>

      <pre><code class="language-bash">npm install openai @anthropic-ai/sdk</code></pre>

      <h2 id="rag-pipeline">Pipeline 1: RAG (Retrieval-Augmented Generation)</h2>

      <p>RAG is the most common AI pattern. It involves:</p>

      <ol>
        <li>Converting a query to an embedding</li>
        <li>Searching a vector database for similar documents</li>
        <li>Generating a response using the retrieved context</li>
      </ol>

      <pre><code class="language-plaintext">â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Embed  â”‚ â”€â”€â–¶ â”‚ Search  â”‚ â”€â”€â–¶ â”‚ Generate â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</code></pre>

      <p>With flashQ, we can model this as a job flow with dependencies:</p>

      <pre><code class="language-typescript">import { Queue, Worker } from 'flashq';
import OpenAI from 'openai';

const openai = new OpenAI();
const queue = new Queue('rag-pipeline');

// Step 1: Embed the query
const embedJob = await queue.add('embed', {
  text: 'What is the capital of France?',
  model: 'text-embedding-3-small'
});

// Step 2: Search (depends on embed)
const searchJob = await queue.add('search', {
  collection: 'knowledge-base',
  topK: 5
}, {
  depends_on: [embedJob.id]
});

// Step 3: Generate (depends on search)
const generateJob = await queue.add('generate', {
  model: 'gpt-4',
  systemPrompt: 'Answer based on the provided context.'
}, {
  depends_on: [searchJob.id]
});

// Wait for the final result
const result = await queue.finished(generateJob.id);
console.log(result); // "The capital of France is Paris."</code></pre>

      <p>Now let's implement the workers:</p>

      <pre><code class="language-typescript">// Embed Worker
new Worker('rag-pipeline', async (job) => {
  if (job.name !== 'embed') return;

  const { text, model } = job.data;

  const response = await openai.embeddings.create({
    input: text,
    model: model
  });

  return {
    embedding: response.data[0].embedding,
    originalText: text
  };
});

// Search Worker
new Worker('rag-pipeline', async (job) => {
  if (job.name !== 'search') return;

  // Get embedding from parent job
  const embedResult = await queue.getResult(job.data.parentId);

  // Search vector database (using your preferred DB)
  const results = await vectorDB.search({
    vector: embedResult.embedding,
    topK: job.data.topK
  });

  return {
    documents: results,
    query: embedResult.originalText
  };
});

// Generate Worker
new Worker('rag-pipeline', async (job) => {
  if (job.name !== 'generate') return;

  const searchResult = await queue.getResult(job.data.parentId);

  const context = searchResult.documents
    .map(d => d.content)
    .join('\n\n');

  const response = await openai.chat.completions.create({
    model: job.data.model,
    messages: [
      { role: 'system', content: job.data.systemPrompt },
      { role: 'user', content: `Context:\n${context}\n\nQuestion: ${searchResult.query}` }
    ]
  });

  return response.choices[0].message.content;
});</code></pre>

      <h2 id="document-processing">Pipeline 2: Document Processing</h2>

      <p>Processing documents for a knowledge base involves multiple steps:</p>

      <pre><code class="language-plaintext">â”Œâ”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”
â”‚ Parse â”‚ â”€â”€â–¶ â”‚ Chunk â”‚ â”€â”€â–¶ â”‚ Embed â”‚ â”€â”€â–¶ â”‚ Store â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”˜</code></pre>

      <p>Here's how to implement it with rate limiting to avoid hitting API quotas:</p>

      <pre><code class="language-typescript">const queue = new Queue('document-processing');

// Set rate limit: 100 requests per minute for embeddings
await queue.setRateLimit(100);

// Process a document
async function processDocument(documentUrl) {
  // Step 1: Parse
  const parseJob = await queue.add('parse', {
    url: documentUrl
  });

  // Step 2: Chunk (depends on parse)
  const chunkJob = await queue.add('chunk', {
    chunkSize: 500,
    overlap: 50
  }, {
    depends_on: [parseJob.id]
  });

  // Wait for chunks to be ready
  const chunks = await queue.finished(chunkJob.id);

  // Step 3: Embed each chunk (rate limited!)
  const embedJobs = await Promise.all(
    chunks.map((chunk, i) =>
      queue.add('embed-chunk', {
        text: chunk.text,
        metadata: chunk.metadata,
        index: i
      })
    )
  );

  // Step 4: Store (depends on all embeddings)
  const storeJob = await queue.add('store', {
    documentUrl,
    totalChunks: chunks.length
  }, {
    depends_on: embedJobs.map(j => j.id)
  });

  return queue.finished(storeJob.id);
}</code></pre>

      <h2 id="batch-inference">Pipeline 3: Batch Inference</h2>

      <p>For batch processing millions of items, flashQ's high throughput really shines:</p>

      <pre><code class="language-typescript">const queue = new Queue('batch-inference');

// Process 1 million items
async function batchProcess(items) {
  console.log(`Processing ${items.length} items...`);

  // Push all jobs in batches of 1000
  const batchSize = 1000;
  const jobIds = [];

  for (let i = 0; i < items.length; i += batchSize) {
    const batch = items.slice(i, i + batchSize);

    // Batch push for maximum throughput
    const jobs = await queue.addBulk(
      batch.map(item => ({
        name: 'predict',
        data: item
      }))
    );

    jobIds.push(...jobs.map(j => j.id));

    // Report progress
    console.log(`Queued ${Math.min(i + batchSize, items.length)} / ${items.length}`);
  }

  console.log('All jobs queued. Processing...');
}

// Worker with concurrency control
new Worker('batch-inference', async (job) => {
  const prediction = await model.predict(job.data);

  // Update progress
  await job.updateProgress(100);

  return prediction;
}, {
  concurrency: 10 // Process 10 jobs in parallel
});</code></pre>

      <h2 id="handling-failures">Handling Failures</h2>

      <p>AI APIs are unreliable. Here's how to handle failures gracefully:</p>

      <pre><code class="language-typescript">// Configure retries with exponential backoff
await queue.add('llm-call', { prompt }, {
  attempts: 5,
  backoff: {
    type: 'exponential',
    delay: 1000  // 1s, 2s, 4s, 8s, 16s
  }
});

// Handle specific errors in worker
new Worker('llm-call', async (job) => {
  try {
    return await openai.chat.completions.create({...});
  } catch (error) {
    if (error.status === 429) {
      // Rate limited - throw to retry
      throw new Error('Rate limited, will retry');
    }
    if (error.status === 400) {
      // Bad request - don't retry
      return { error: 'Invalid request', skip: true };
    }
    throw error;
  }
});

// Monitor the dead letter queue
const failedJobs = await queue.getDlq(100);
console.log(`${failedJobs.length} jobs in DLQ`);

// Retry failed jobs
await queue.retryDlq();</code></pre>

      <h2 id="monitoring">Monitoring and Observability</h2>

      <p>Track your pipelines in real-time:</p>

      <pre><code class="language-typescript">// Get queue stats
const stats = await queue.getJobCounts();
console.log(stats);
// {
//   waiting: 1523,
//   active: 10,
//   completed: 45230,
//   failed: 12,
//   delayed: 0
// }

// Track progress of a specific job
const progress = await queue.getProgress(jobId);
console.log(`Job ${jobId}: ${progress.percent}% - ${progress.message}`);

// Listen to events
queue.on('completed', (job, result) => {
  console.log(`Job ${job.id} completed`);
  metrics.increment('jobs.completed');
});

queue.on('failed', (job, error) => {
  console.error(`Job ${job.id} failed: ${error.message}`);
  metrics.increment('jobs.failed');
});</code></pre>

      <div class="callout callout-success">
        <div class="callout-title">ğŸ’¡ Pro Tip</div>
        <p>flashQ exposes Prometheus metrics at <code>/metrics/prometheus</code> when running with HTTP enabled. Use Grafana to visualize queue health.</p>
      </div>

      <h2 id="best-practices">Best Practices</h2>

      <h3>1. Use Job Dependencies for Complex Pipelines</h3>

      <p>Instead of polling for job completion, use <code>depends_on</code> to chain jobs. This is more efficient and lets flashQ optimize scheduling.</p>

      <h3>2. Set Appropriate Timeouts</h3>

      <p>LLM calls can be slow. Set timeouts that match your use case:</p>

      <pre><code class="language-typescript">await queue.add('generate', data, {
  timeout: 60000  // 60 seconds for LLM calls
});</code></pre>

      <h3>3. Use Rate Limiting to Control Costs</h3>

      <p>Rate limiting isn't just for API complianceâ€”it's for budget control:</p>

      <pre><code class="language-typescript">// Limit to $10/hour in API calls
// GPT-4 â‰ˆ $0.03 per 1K tokens â‰ˆ 333 calls per $10
await queue.setRateLimit(333); // 333 per hour = ~6 per minute</code></pre>

      <h3>4. Track Costs Per Job</h3>

      <p>Store token usage in job results for cost tracking:</p>

      <pre><code class="language-typescript">return {
  result: response.choices[0].message.content,
  usage: {
    promptTokens: response.usage.prompt_tokens,
    completionTokens: response.usage.completion_tokens,
    cost: calculateCost(response.usage)
  }
};</code></pre>

      <h3>5. Use Concurrency Control</h3>

      <p>Match concurrency to your API limits and system resources:</p>

      <pre><code class="language-typescript">new Worker('embeddings', processor, {
  concurrency: 5  // 5 parallel embedding calls
});

// Or set at queue level
await queue.setConcurrency(10);</code></pre>

      <h2 id="conclusion">Conclusion</h2>

      <p>Building reliable AI pipelines doesn't have to be complicated. With flashQ, you get:</p>

      <ul>
        <li><strong>Job dependencies</strong> for complex workflows</li>
        <li><strong>Rate limiting</strong> to control API costs</li>
        <li><strong>Automatic retries</strong> for transient failures</li>
        <li><strong>Progress tracking</strong> for long-running jobs</li>
        <li><strong>High throughput</strong> for batch processing</li>
      </ul>

      <p>All without managing Redis or any external infrastructure.</p>

      <p>Ready to build your own AI pipeline? Check out the <a href="../docs/">documentation</a> for the full API reference and more examples.</p>

      <!-- CTA -->
      <div class="article-cta">
        <h3>Start Building</h3>
        <p>Get flashQ running in 5 minutes and start building AI pipelines.</p>
        <a href="../docs/#quickstart" class="btn btn-primary">Get Started â†’</a>
      </div>
        </div><!-- /.article-main -->

        <!-- Table of Contents -->
        <aside class="toc-sidebar">
          <nav class="toc">
            <div class="toc-title">On this page</div>
            <ul class="toc-list">
              <li><a href="#why-job-queue">Why Use a Job Queue?</a></li>
              <li><a href="#setup">Setup</a></li>
              <li><a href="#rag-pipeline">RAG Pipeline</a></li>
              <li><a href="#document-processing">Document Processing</a></li>
              <li><a href="#batch-inference">Batch Inference</a></li>
              <li><a href="#handling-failures">Handling Failures</a></li>
              <li><a href="#monitoring">Monitoring</a></li>
              <li><a href="#best-practices">Best Practices</a></li>
              <li><a href="#conclusion">Conclusion</a></li>
            </ul>
          </nav>
        </aside>
      </div><!-- /.article-layout -->
    </div>
  </article>

  <!-- Footer -->
  <footer>
    <div class="container wide">
      <a href="../" class="logo">
        <span>âš¡</span> flashQ
      </a>
      <div class="footer-links">
        <a href="https://github.com/egeominotti/flashq" target="_blank">GitHub</a>
        <a href="https://npmjs.com/package/flashq" target="_blank">npm</a>
        <a href="../docs/">Docs</a>
        <a href="../blog/">Blog</a>
      </div>
      <div class="footer-copy">
        Â© <span id="year"></span> flashQ. MIT License.
      </div>
    </div>
  </footer>
  <!-- Search Modal -->
  <div class="search-overlay" id="searchOverlay" onclick="closeSearch(event)">
    <div class="search-modal" onclick="event.stopPropagation()">
      <div class="search-input-wrapper">
        <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M21 21l-6-6m2-5a7 7 0 11-14 0 7 7 0 0114 0z"/></svg>
        <input type="text" class="search-modal-input" id="searchInput" placeholder="Search articles and docs..." autofocus>
        <span class="search-shortcut">ESC</span>
      </div>
      <div class="search-results" id="searchResults"></div>
      <div class="search-footer">
        <span><kbd>â†‘</kbd><kbd>â†“</kbd> to navigate</span>
        <span><kbd>Enter</kbd> to select</span>
      </div>
    </div>
  </div>

  <script>
    const mobileMenuBtn = document.querySelector(".mobile-menu-btn");
    const mobileMenu = document.querySelector(".mobile-menu");
    document.getElementById("year").textContent = new Date().getFullYear();
    mobileMenuBtn.addEventListener("click", () => {
      mobileMenuBtn.classList.toggle("active");
      mobileMenu.classList.toggle("active");
      document.body.style.overflow = mobileMenu.classList.contains("active") ? "hidden" : "";
    });
    mobileMenu.querySelectorAll("a").forEach(link => {
      link.addEventListener("click", () => {
        mobileMenuBtn.classList.remove("active");
        mobileMenu.classList.remove("active");
        document.body.style.overflow = "";
      });
    });

    // TOC active state
    const tocLinks = document.querySelectorAll('.toc-list a');
    const headings = document.querySelectorAll('h2[id]');

    function updateTocActive() {
      let current = '';
      headings.forEach(heading => {
        if (window.scrollY >= heading.offsetTop - 120) {
          current = heading.getAttribute('id');
        }
      });
      tocLinks.forEach(link => {
        link.classList.toggle('active', link.getAttribute('href') === '#' + current);
      });
    }
    window.addEventListener('scroll', updateTocActive);
    updateTocActive();

    // Search functionality
    const searchData = [
      { title: "flashQ in Action: Live Demo", url: "flashq-in-action.html", description: "Interactive walkthrough of all flashQ features", tag: "Tutorial" },
      { title: "flashQ Architecture", url: "flashq-architecture.html", description: "Deep dive into flashQ's 1.9M jobs/sec design", tag: "Deep Dive" },
      { title: "Rate Limiting OpenAI", url: "rate-limiting-openai.html", description: "How to rate limit AI API calls with flashQ", tag: "Tutorial" },
      { title: "Job Queue Patterns", url: "job-queue-patterns.html", description: "Essential patterns for AI applications", tag: "Best Practices" },
      { title: "Building AI Pipelines", url: "building-ai-pipelines.html", description: "Complete guide to RAG and LLM workflows", tag: "Tutorial" },
      { title: "flashQ vs BullMQ", url: "flashq-vs-bullmq.html", description: "Why we built a Redis-free alternative", tag: "Comparison" },
      { title: "Monitoring AI Pipelines", url: "monitoring-ai-pipelines.html", description: "Metrics, alerts, and dashboards setup", tag: "Tutorial" },
      { title: "What is flashQ?", url: "what-is-flashq.html", description: "Introduction to flashQ job queue", tag: "Introduction" },
      { title: "Documentation", url: "../docs/", description: "Complete API reference and guides", tag: "Docs" },
      { title: "Quick Start", url: "../docs/#quickstart", description: "Get started with flashQ in 5 minutes", tag: "Docs" }
    ];

    let selectedIndex = 0;

    function openSearch() {
      document.getElementById('searchOverlay').classList.add('active');
      document.getElementById('searchInput').focus();
      document.body.style.overflow = 'hidden';
      renderResults('');
    }

    function closeSearch(e) {
      if (e && e.target !== document.getElementById('searchOverlay')) return;
      document.getElementById('searchOverlay').classList.remove('active');
      document.getElementById('searchInput').value = '';
      document.body.style.overflow = '';
    }

    function renderResults(query) {
      const results = query
        ? searchData.filter(item =>
            item.title.toLowerCase().includes(query.toLowerCase()) ||
            item.description.toLowerCase().includes(query.toLowerCase())
          )
        : searchData;

      selectedIndex = 0;
      const container = document.getElementById('searchResults');

      if (results.length === 0) {
        container.innerHTML = '<div class="search-empty">No results found</div>';
        return;
      }

      container.innerHTML = results.map((item, i) => `
        <a href="${item.url}" class="search-result ${i === 0 ? 'active' : ''}" data-index="${i}">
          <div class="search-result-title">${item.title}</div>
          <div class="search-result-description">${item.description}</div>
          <span class="search-result-tag">${item.tag}</span>
        </a>
      `).join('');
    }

    document.getElementById('searchInput').addEventListener('input', (e) => {
      renderResults(e.target.value);
    });

    document.addEventListener('keydown', (e) => {
      const overlay = document.getElementById('searchOverlay');
      if ((e.metaKey || e.ctrlKey) && e.key === 'k') {
        e.preventDefault();
        openSearch();
      }
      if (e.key === 'Escape' && overlay.classList.contains('active')) {
        closeSearch({target: overlay});
      }
      if (overlay.classList.contains('active')) {
        const results = document.querySelectorAll('.search-result');
        if (e.key === 'ArrowDown') {
          e.preventDefault();
          selectedIndex = Math.min(selectedIndex + 1, results.length - 1);
          results.forEach((r, i) => r.classList.toggle('active', i === selectedIndex));
        }
        if (e.key === 'ArrowUp') {
          e.preventDefault();
          selectedIndex = Math.max(selectedIndex - 1, 0);
          results.forEach((r, i) => r.classList.toggle('active', i === selectedIndex));
        }
        if (e.key === 'Enter') {
          e.preventDefault();
          results[selectedIndex]?.click();
        }
      }
    });
  </script>
  <script>hljs.highlightAll();</script>
</body>
</html>
