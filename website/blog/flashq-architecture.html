<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>flashQ Architecture: How We Built a 1.9M Jobs/sec Queue Server - flashQ Blog</title>
  <meta name="description" content="Deep dive into flashQ's architecture: sharded design, lock-free data structures, and optimizations that achieve 1.9M jobs/sec throughput.">
  <meta name="keywords" content="flashq architecture, high performance queue, rust job queue, sharded architecture, lock-free data structures">
  <meta name="robots" content="index, follow">

  <meta property="og:title" content="flashQ Architecture: How We Built a 1.9M Jobs/sec Queue Server">
  <meta property="og:description" content="Deep dive into flashQ's architecture and performance optimizations.">
  <meta property="og:type" content="article">
  <meta property="og:url" content="https://flashq.dev/blog/flashq-architecture.html">
  <meta property="og:image" content="https://flashq.dev/og-image.png">

  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="flashQ Architecture: How We Built a 1.9M Jobs/sec Queue Server">
  <meta name="twitter:image" content="https://flashq.dev/og-image.png">

  <meta property="article:published_time" content="2025-11-25">

  <link rel="canonical" href="https://flashq.dev/blog/flashq-architecture.html">
  <link rel="icon" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='.9em' font-size='90'>âš¡</text></svg>">

  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800;900&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">

  <link rel="stylesheet" href="styles.css">

  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Article",
    "headline": "flashQ Architecture: How We Built a 1.9M Jobs/sec Queue Server",
    "datePublished": "2025-11-25",
    "author": { "@type": "Organization", "name": "flashQ" }
  }
  </script>

  <!-- Breadcrumb Schema -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "BreadcrumbList",
    "itemListElement": [
      { "@type": "ListItem", "position": 1, "name": "Home", "item": "https://flashq.dev" },
      { "@type": "ListItem", "position": 2, "name": "Blog", "item": "https://flashq.dev/blog/" },
      { "@type": "ListItem", "position": 3, "name": "flashQ Architecture", "item": "https://flashq.dev/blog/flashq-architecture.html" }
    ]
  }
  </script>
  <!-- Highlight.js -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/typescript.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/bash.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/yaml.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/json.min.js"></script>
</head>
<body>
  <nav>
    <div class="container wide">
      <a href="../" class="logo"><span>âš¡</span> flashQ</a>
      <div class="nav-links">
        <a href="../#features">Features</a>
        <a href="../blog/" class="active">Blog</a>
        <a href="../docs/">Docs</a>
        <a href="https://github.com/egeominotti/flashq" target="_blank">GitHub</a>
        <a href="../docs/#quickstart" class="btn btn-primary">Get Started</a>
      </div>
      <button class="mobile-menu-btn" aria-label="Menu">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
  </nav>

  <div class="mobile-menu">
    <a href="../#features">Features</a>
    <a href="../blog/">Blog</a>
    <a href="../docs/">Docs</a>
    <a href="https://github.com/egeominotti/flashq" target="_blank">GitHub</a>
    <a href="../docs/#quickstart" class="btn btn-primary">Get Started</a>
  </div>

  <header class="article-header header-devops">
    <div class="container">
      <span class="article-tag">Deep Dive</span>
      <h1>flashQ Architecture: How We Built a 1.9M Jobs/sec Queue Server</h1>
      <div class="article-meta">
        <span>ğŸ“… November 25, 2025</span>
        <span>â±ï¸ 15 min read</span>
      </div>
    </div>
  </header>

  <article class="article-content">
    <div class="container">
      <p>When we set out to build flashQ, our goal was simple: create the fastest job queue server possible, optimized for AI workloads, without requiring Redis. In this article, we'll dive deep into the architecture decisions that enable flashQ to process <strong>1.9 million jobs per second</strong>.</p>

      <h2>High-Level Architecture</h2>

      <p>flashQ is written in Rust and follows a sharded, lock-free architecture designed for maximum concurrency:</p>

      <pre><code class="language-plaintext">â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      flashQ Server                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚ TCP/IP  â”‚  â”‚  HTTP   â”‚  â”‚  gRPC   â”‚  â”‚  Unix   â”‚       â”‚
â”‚  â”‚ Handler â”‚  â”‚   API   â”‚  â”‚   API   â”‚  â”‚ Socket  â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜       â”‚
â”‚       â”‚            â”‚            â”‚            â”‚             â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                         â”‚                                   â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚              â”‚   Command Router    â”‚                       â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚                         â”‚                                   â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚         â–¼               â–¼               â–¼                  â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚   â”‚ Shard 0  â”‚   â”‚ Shard 1  â”‚...â”‚ Shard 31 â”‚              â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                         â”‚                                   â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚              â”‚   DashMap Index     â”‚                       â”‚
â”‚              â”‚  (Lock-Free O(1))   â”‚                       â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</code></pre>

      <h2>The 32-Shard Design</h2>

      <p>At the heart of flashQ is a sharded architecture. We distribute queues across <strong>32 shards</strong> based on a hash of the queue name. This reduces lock contention by 97% compared to a single-lock design.</p>

      <pre><code class="language-typescript">// Queue name â†’ Shard mapping
fn get_shard(queue_name: &str) -> usize {
    let hash = fxhash::hash64(queue_name.as_bytes());
    (hash as usize) % 32
}</code></pre>

      <p>Each shard contains:</p>

      <ul>
        <li><strong>Queues</strong>: HashMap of queue name â†’ IndexedPriorityQueue</li>
        <li><strong>Processing</strong>: Jobs currently being processed</li>
        <li><strong>Completed</strong>: Recently completed job IDs</li>
        <li><strong>DLQ</strong>: Dead letter queue for failed jobs</li>
        <li><strong>Results</strong>: Job results for the <code>finished()</code> API</li>
      </ul>

      <h2>Lock-Free Job Index with DashMap</h2>

      <p>One of our biggest performance wins came from using <code>DashMap</code> for the global job index. DashMap is a concurrent HashMap that uses fine-grained locking, allowing multiple threads to read and write simultaneously.</p>

      <pre><code class="language-typescript">// Global job index - O(1) lookups
pub struct QueueManager {
    shards: [RwLock&lt;Shard&gt;; 32],
    job_index: DashMap&lt;u64, JobLocation&gt;,  // Lock-free!
}

// JobLocation tells us exactly where a job is
enum JobLocation {
    Waiting { shard: u8, queue: CompactString },
    Processing { shard: u8 },
    Completed { shard: u8 },
    Failed { shard: u8 },
}</code></pre>

      <p>This gives us <strong>O(1) job lookups</strong> regardless of how many queues or jobs exist. Operations like <code>getJob()</code>, <code>cancel()</code>, and <code>getState()</code> are blazing fast.</p>

      <h2>IndexedPriorityQueue for O(log n) Operations</h2>

      <p>Standard binary heaps don't support efficient removal of arbitrary elements. We built an <code>IndexedPriorityQueue</code> that maintains a secondary index:</p>

      <pre><code class="language-typescript">pub struct IndexedPriorityQueue&lt;T&gt; {
    heap: Vec&lt;T&gt;,
    index: HashMap&lt;u64, usize&gt;,  // job_id â†’ heap position
}

impl&lt;T&gt; IndexedPriorityQueue&lt;T&gt; {
    // All operations are O(log n)
    fn push(&mut self, job: T) { ... }
    fn pop(&mut self) -> Option&lt;T&gt; { ... }
    fn remove(&mut self, job_id: u64) -> Option&lt;T&gt; { ... }  // Key!
    fn update_priority(&mut self, job_id: u64, priority: i32) { ... }
}</code></pre>

      <p>This enables efficient <code>cancel()</code>, <code>promote()</code>, and <code>changePriority()</code> operations that would be O(n) with a standard heap.</p>

      <h2>CompactString for Zero-Allocation Queue Names</h2>

      <p>Queue names are accessed constantly. We use <code>CompactString</code> which stores strings up to 24 bytes inline (no heap allocation):</p>

      <pre><code class="language-typescript">// Most queue names fit in 24 bytes
use compact_str::CompactString;

// "embeddings" - 10 bytes, stored inline âœ“
// "user-notifications" - 18 bytes, stored inline âœ“
// "very-long-queue-name-here" - 25 bytes, heap allocated</code></pre>

      <p>This reduces memory allocations by ~60% for typical workloads.</p>

      <h2>Sharded Processing Map</h2>

      <p>Jobs being processed are distributed across 32 shards (separate from queue shards). This reduces contention during <code>ack()</code> and <code>fail()</code> operations by 97%:</p>

      <pre><code class="language-typescript">// Processing is sharded by job_id
fn get_processing_shard(job_id: u64) -> usize {
    (job_id as usize) % 32
}

// ack() only locks one of 32 processing shards
fn ack(&self, job_id: u64) -> Result&lt;()&gt; {
    let shard_idx = get_processing_shard(job_id);
    let mut processing = self.processing[shard_idx].write();
    // ...
}</code></pre>

      <h2>Binary Protocol with MessagePack</h2>

      <p>flashQ supports both JSON and MessagePack protocols. MessagePack provides:</p>

      <ul>
        <li><strong>40% smaller</strong> payloads on the wire</li>
        <li><strong>3-5x faster</strong> serialization/deserialization</li>
        <li>Full type safety and schema compatibility</li>
      </ul>

      <pre><code class="language-typescript">// TypeScript SDK - enable binary protocol
const client = new FlashQ({
  host: 'localhost',
  port: 6789,
  useBinary: true  // Use MessagePack
});</code></pre>

      <h2>Memory Management</h2>

      <p>We use several strategies to prevent unbounded memory growth:</p>

      <h3>Completed Jobs Cleanup</h3>
      <pre><code class="language-typescript">// When completed_jobs exceeds 50K, remove oldest 25K
if completed_jobs.len() > 50_000 {
    let to_remove: Vec&lt;_&gt; = completed_jobs
        .iter()
        .take(25_000)
        .cloned()
        .collect();
    for id in to_remove {
        completed_jobs.remove(&id);
    }
}</code></pre>

      <h3>Job Results TTL</h3>
      <pre><code class="language-typescript">// Results are cleaned up based on keepCompletedAge
await client.push('queue', data, {
  keepCompletedAge: 86400000,  // Keep result for 24h
  keepCompletedCount: 100     // Or keep in last 100
});</code></pre>

      <h3>String Interning</h3>
      <pre><code class="language-typescript">// Queue names are interned to reduce allocations
// Limited to 10K unique names to prevent memory exhaustion
static INTERNED: DashMap&lt;String, CompactString&gt; = ...;</code></pre>

      <h2>Background Tasks</h2>

      <p>flashQ runs several background tasks at different intervals:</p>

      <table>
        <thead>
          <tr>
            <th>Task</th>
            <th>Interval</th>
            <th>Purpose</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Wakeup</td>
            <td>100ms</td>
            <td>Notify workers, check dependencies</td>
          </tr>
          <tr>
            <td>Timeout</td>
            <td>500ms</td>
            <td>Check and fail stalled jobs</td>
          </tr>
          <tr>
            <td>Cron</td>
            <td>1s</td>
            <td>Execute scheduled cron jobs</td>
          </tr>
          <tr>
            <td>Metrics</td>
            <td>5s</td>
            <td>Collect metrics history</td>
          </tr>
          <tr>
            <td>Cleanup</td>
            <td>60s</td>
            <td>Clean completed jobs, results, index</td>
          </tr>
        </tbody>
      </table>

      <h2>Multi-Protocol Support</h2>

      <p>flashQ supports four connection methods simultaneously:</p>

      <pre><code class="language-bash"># TCP (default) - lowest latency
PORT=6789 ./flashq-server

# HTTP REST API + WebSocket
HTTP=1 HTTP_PORT=6790 ./flashq-server

# gRPC API
GRPC=1 GRPC_PORT=6791 ./flashq-server

# Unix Socket - highest throughput for local
UNIX_SOCKET=1 ./flashq-server</code></pre>

      <h2>Clustering Architecture</h2>

      <p>For high availability, flashQ supports clustering using PostgreSQL for coordination:</p>

      <pre><code class="language-plaintext">â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Node 1  â”‚    â”‚  Node 2  â”‚    â”‚  Node 3  â”‚
â”‚ (Leader) â”‚    â”‚(Follower)â”‚    â”‚(Follower)â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚               â”‚               â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
              â”‚  PostgreSQL â”‚
              â”‚  (Shared)   â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</code></pre>

      <p>Leader election uses PostgreSQL advisory locks (<code>pg_try_advisory_lock</code>). Only the leader runs background tasks; all nodes handle client requests.</p>

      <h2>Performance Optimizations Summary</h2>

      <table>
        <thead>
          <tr>
            <th>Optimization</th>
            <th>Benefit</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>DashMap job_index</td>
            <td>Lock-free O(1) lookups, 40% faster</td>
          </tr>
          <tr>
            <td>32 Sharded processing</td>
            <td>-97% contention on ack/fail</td>
          </tr>
          <tr>
            <td>CompactString</td>
            <td>Zero heap alloc for short names</td>
          </tr>
          <tr>
            <td>IndexedPriorityQueue</td>
            <td>O(log n) cancel/update/promote</td>
          </tr>
          <tr>
            <td>MessagePack protocol</td>
            <td>40% smaller, 3-5x faster serialization</td>
          </tr>
          <tr>
            <td>mimalloc allocator</td>
            <td>Faster memory allocation</td>
          </tr>
          <tr>
            <td>parking_lot locks</td>
            <td>Faster than std::sync</td>
          </tr>
          <tr>
            <td>Atomic u64 IDs</td>
            <td>Lock-free ID generation</td>
          </tr>
          <tr>
            <td>Coarse timestamps</td>
            <td>Cached time, fewer syscalls</td>
          </tr>
          <tr>
            <td>LTO build</td>
            <td>Cross-crate optimization</td>
          </tr>
        </tbody>
      </table>

      <h2>Benchmarks</h2>

      <p>On a modern server (32 cores, 64GB RAM):</p>

      <table>
        <thead>
          <tr>
            <th>Operation</th>
            <th>Throughput</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Push (batch)</td>
            <td>1.9M jobs/sec</td>
          </tr>
          <tr>
            <td>Processing (no-op)</td>
            <td>280K jobs/sec</td>
          </tr>
          <tr>
            <td>Processing (CPU work)</td>
            <td>196K jobs/sec</td>
          </tr>
          <tr>
            <td>Concurrent push (10 conn)</td>
            <td>59K ops/sec</td>
          </tr>
        </tbody>
      </table>

      <h2>Conclusion</h2>

      <p>flashQ's architecture is built around one principle: <strong>minimize contention</strong>. By sharding data, using lock-free structures where possible, and optimizing hot paths, we've created a job queue that can handle the most demanding AI workloads.</p>

      <p>The combination of Rust's zero-cost abstractions, careful data structure selection, and attention to memory management allows flashQ to outperform traditional Redis-based solutions while being simpler to operate.</p>

      <div class="article-cta">
        <h3>Try flashQ</h3>
        <p>Experience the performance for yourself.</p>
        <a href="../docs/#quickstart" class="btn btn-primary">Get Started â†’</a>
      </div>
    </div>
  </article>

  <footer>
    <div class="container wide">
      <a href="../" class="logo"><span>âš¡</span> flashQ</a>
      <div class="footer-links">
        <a href="https://github.com/egeominotti/flashq" target="_blank">GitHub</a>
        <a href="https://npmjs.com/package/flashq" target="_blank">npm</a>
        <a href="../docs/">Docs</a>
        <a href="../blog/">Blog</a>
      </div>
      <div class="footer-copy">Â© <span id="year"></span> flashQ. MIT License.</div>
    </div>
  </footer>

  <script>
    const mobileMenuBtn = document.querySelector(".mobile-menu-btn");
    const mobileMenu = document.querySelector(".mobile-menu");
    document.getElementById("year").textContent = new Date().getFullYear();
    mobileMenuBtn.addEventListener("click", () => {
      mobileMenuBtn.classList.toggle("active");
      mobileMenu.classList.toggle("active");
      document.body.style.overflow = mobileMenu.classList.contains("active") ? "hidden" : "";
    });
    mobileMenu.querySelectorAll("a").forEach(link => {
      link.addEventListener("click", () => {
        mobileMenuBtn.classList.remove("active");
        mobileMenu.classList.remove("active");
        document.body.style.overflow = "";
      });
    });
  </script>
  <script>hljs.highlightAll();</script>
</body>
</html>
